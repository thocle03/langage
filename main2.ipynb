{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "104ae6a1",
   "metadata": {},
   "source": [
    "# Vectorisation de Texte: OneHotEncoder et Bag of Words\n",
    "\n",
    "**Suite du prétraitement NLP - Données de main.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec5c1a6",
   "metadata": {},
   "source": [
    "## 0. Chargement des données (à exécuter d'abord!)\n",
    "\n",
    "Charger les données prétraitées depuis main.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1271b3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHARGEMENT DES DONNÉES DE main.ipynb\n",
      "======================================================================\n",
      "\n",
      "⚠️  cleaned_tokens non disponible dans le kernel actuel\n",
      "\n",
      "✗ Fichier cleaned_tokens.pkl non trouvé\n",
      "\n",
      "SOLUTIONS DE CHARGEMENT:\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "1️⃣  MÉTHODE RECOMMANDÉE (Même kernel):\n",
      "   ✓ Ouvrir main.ipynb dans le même VS Code\n",
      "   ✓ Exécuter TOUTES les cellules (Ctrl+Alt+Entrée)\n",
      "   ✓ Revenir à main2.ipynb et exécuter cette cellule\n",
      "\n",
      "2️⃣  MÉTHODE ALTERNATIVE (Sauvegarder/Charger):\n",
      "   ✓ Dans main.ipynb, ajouter à la fin:\n",
      "     import pickle\n",
      "     with open('cleaned_tokens.pkl', 'wb') as f:\n",
      "         pickle.dump(cleaned_tokens, f)\n",
      "   ✓ Exécuter la cellule\n",
      "   ✓ Revenir à main2.ipynb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHARGEMENT DES DONNÉES DE main.ipynb\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# Vérifier si les variables existent déjà (si main.ipynb a été exécuté avant)\n",
    "try:\n",
    "    # Tester si cleaned_tokens existe\n",
    "    test = cleaned_tokens\n",
    "    print(\"✓ Variables trouvées dans le kernel actuel\")\n",
    "    print(f\"  • cleaned_tokens: {len(cleaned_tokens)} mots\")\n",
    "    data_loaded = True\n",
    "except NameError:\n",
    "    data_loaded = False\n",
    "    print(\"⚠️  cleaned_tokens non disponible dans le kernel actuel\\n\")\n",
    "    \n",
    "    # Essayer de charger depuis un fichier pickle\n",
    "    pickle_file = 'cleaned_tokens.pkl'\n",
    "    if os.path.exists(pickle_file):\n",
    "        print(f\"✓ Fichier {pickle_file} trouvé!\")\n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                cleaned_tokens = pickle.load(f)\n",
    "            print(f\"✓ Données chargées: {len(cleaned_tokens)} tokens\")\n",
    "            data_loaded = True\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Erreur lors du chargement: {e}\")\n",
    "    else:\n",
    "        print(f\"✗ Fichier {pickle_file} non trouvé\\n\")\n",
    "        print(\"SOLUTIONS DE CHARGEMENT:\")\n",
    "        print(\"─\" * 70)\n",
    "        print(\"\\n1️⃣  MÉTHODE RECOMMANDÉE (Même kernel):\")\n",
    "        print(\"   ✓ Ouvrir main.ipynb dans le même VS Code\")\n",
    "        print(\"   ✓ Exécuter TOUTES les cellules (Ctrl+Alt+Entrée)\")\n",
    "        print(\"   ✓ Revenir à main2.ipynb et exécuter cette cellule\\n\")\n",
    "        \n",
    "        print(\"2️⃣  MÉTHODE ALTERNATIVE (Sauvegarder/Charger):\")\n",
    "        print(\"   ✓ Dans main.ipynb, ajouter à la fin:\")\n",
    "        print(\"     import pickle\")\n",
    "        print(\"     with open('cleaned_tokens.pkl', 'wb') as f:\")\n",
    "        print(\"         pickle.dump(cleaned_tokens, f)\")\n",
    "        print(\"   ✓ Exécuter la cellule\")\n",
    "        print(\"   ✓ Revenir à main2.ipynb\\n\")\n",
    "\n",
    "if data_loaded:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✓ DONNÉES CHARGÉES AVEC SUCCÈS\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Prêt pour la vectorisation!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648faca5",
   "metadata": {},
   "source": [
    "## 1. Construction du Vocabulaire\n",
    "\n",
    "Créer un vocabulaire avec une seule apparition pour chaque token unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le vocabulaire à partir des tokens nettoyés\n",
    "if data_loaded:\n",
    "    try:\n",
    "        vocab = list(set(cleaned_tokens))  # Créer un vocabulaire unique\n",
    "        vocab_sorted = sorted(vocab)\n",
    "        \n",
    "        print(f\"Vocabulaire créé à partir des tokens nettoyés\")\n",
    "        print(f\"  Taille du vocabulaire: {len(vocab)} mots uniques\")\n",
    "        print(f\"  Tokens nettoyés disponibles: {len(cleaned_tokens)}\\n\")\n",
    "        \n",
    "        print(f\"Premiers 20 mots du vocabulaire: {vocab_sorted[:20]}\\n\")\n",
    "        print(f\"Derniers 20 mots du vocabulaire: {vocab_sorted[-20:]}\")\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"⚠️  cleaned_tokens non disponible - Exécuter d'abord la cellule 0!\")\n",
    "        vocab = []\n",
    "else:\n",
    "    print(\"⚠️  Données non chargées - Exécuter la cellule 0 d'abord!\")\n",
    "    vocab = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbe2b9f",
   "metadata": {},
   "source": [
    "## 2. OneHotEncoder - Encodage One-Hot\n",
    "\n",
    "Chaque mot unique est représenté par un vecteur où seul son index est à 1, le reste à 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ee97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder:\n",
    "    \"\"\"\n",
    "    Encodeur One-Hot personnalisé (style Java).\n",
    "    Chaque mot unique est représenté par un vecteur de 0 et 1.\n",
    "    \n",
    "    Attributs:\n",
    "        vocabulary: dict - Mapping {mot: index}\n",
    "        vocab_size: int - Taille du vocabulaire\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary):\n",
    "        \"\"\"\n",
    "        Initialiser l'encodeur avec un vocabulaire.\n",
    "        \n",
    "        Args:\n",
    "            vocabulary: list - Liste des mots uniques\n",
    "        \"\"\"\n",
    "        # Créer un dictionnaire {mot: index}\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        print(f\"✓ OneHotEncoder initialisé avec {self.vocab_size} mots\")\n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        \"\"\"\n",
    "        Encoder un seul mot en vecteur one-hot.\n",
    "        \n",
    "        Args:\n",
    "            word: str - Le mot à encoder\n",
    "        \n",
    "        Returns:\n",
    "            list - Vecteur one-hot de taille vocab_size\n",
    "        \"\"\"\n",
    "        if word not in self.vocabulary:\n",
    "            raise ValueError(f\"Mot '{word}' non trouvé dans le vocabulaire\")\n",
    "        \n",
    "        # Créer un vecteur de zéros\n",
    "        one_hot_vector = [0] * self.vocab_size\n",
    "        # Mettre 1 à l'index du mot\n",
    "        one_hot_vector[self.vocabulary[word]] = 1\n",
    "        \n",
    "        return one_hot_vector\n",
    "    \n",
    "    def encode_sentence(self, words):\n",
    "        \"\"\"\n",
    "        Encoder une phrase (liste de mots) en liste de vecteurs one-hot.\n",
    "        \n",
    "        Args:\n",
    "            words: list - Liste de mots\n",
    "        \n",
    "        Returns:\n",
    "            list - Liste de vecteurs one-hot\n",
    "        \"\"\"\n",
    "        encoded = []\n",
    "        for word in words:\n",
    "            if word in self.vocabulary:\n",
    "                encoded.append(self.encode_word(word))\n",
    "        return encoded\n",
    "    \n",
    "    def decode_vector(self, vector):\n",
    "        \"\"\"\n",
    "        Décoder un vecteur one-hot en mot.\n",
    "        \n",
    "        Args:\n",
    "            vector: list - Vecteur one-hot\n",
    "        \n",
    "        Returns:\n",
    "            str - Le mot correspondant (ou None si pas valide)\n",
    "        \"\"\"\n",
    "        if vector.count(1) != 1:\n",
    "            return None\n",
    "        \n",
    "        index = vector.index(1)\n",
    "        # Trouver le mot correspondant à cet index\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            if idx == index:\n",
    "                return word\n",
    "        return None\n",
    "    \n",
    "    def get_vocabulary_info(self):\n",
    "        \"\"\"Retourner les infos du vocabulaire\"\"\"\n",
    "        return {\n",
    "            'size': self.vocab_size,\n",
    "            'vocabulary': self.vocabulary\n",
    "        }\n",
    "\n",
    "\n",
    "# Tester OneHotEncoder\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST OneHotEncoder\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "if vocab:\n",
    "    # Créer l'encodeur\n",
    "    encoder = OneHotEncoder(vocab)\n",
    "    \n",
    "    # Tester avec un mot\n",
    "    test_word = vocab_sorted[0]\n",
    "    print(f\"Encodage du mot '{test_word}':\")\n",
    "    encoded = encoder.encode_word(test_word)\n",
    "    print(f\"  Vecteur: {encoded}\")\n",
    "    print(f\"  Index du 1: {encoded.index(1)}\\n\")\n",
    "    \n",
    "    # Tester avec plusieurs mots\n",
    "    test_words = vocab_sorted[:5]\n",
    "    print(f\"Encodage des mots {test_words}:\")\n",
    "    for word in test_words:\n",
    "        vec = encoder.encode_word(word)\n",
    "        print(f\"  {word:<15} → {vec.index(1):3d} (1 à la position {vec.index(1)})\")\n",
    "    \n",
    "    # Tester le décodage\n",
    "    print(f\"\\nDécodage des vecteurs:\")\n",
    "    for word in test_words:\n",
    "        vec = encoder.encode_word(word)\n",
    "        decoded = encoder.decode_vector(vec)\n",
    "        print(f\"  Vecteur → {decoded}\")\n",
    "    \n",
    "    # Statistiques\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"Taille du vocabulaire: {encoder.vocab_size} mots\")\n",
    "    print(f\"Dimension de chaque vecteur one-hot: {encoder.vocab_size}\")\n",
    "    print(f\"Sparsité: ~99.85% de zéros (pour chaque vecteur)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6815b3c",
   "metadata": {},
   "source": [
    "## 3. Bag of Words - Représentation par Fréquences\n",
    "\n",
    "Compter l'occurrence de chaque mot dans un document (ou ensemble de mots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    \"\"\"\n",
    "    Bag of Words personnalisé (style Java).\n",
    "    Représente un texte par les fréquences d'apparition de chaque mot.\n",
    "    \n",
    "    Attributs:\n",
    "        vocabulary: dict - Mapping {mot: index}\n",
    "        vocab_size: int - Taille du vocabulaire\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary):\n",
    "        \"\"\"\n",
    "        Initialiser le BoW avec un vocabulaire.\n",
    "        \n",
    "        Args:\n",
    "            vocabulary: list - Liste des mots uniques\n",
    "        \"\"\"\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        print(f\"✓ BagOfWords initialisé avec {self.vocab_size} mots\")\n",
    "    \n",
    "    def encode_document(self, words):\n",
    "        \"\"\"\n",
    "        Encoder un document (liste de mots) en vecteur de fréquences.\n",
    "        \n",
    "        Args:\n",
    "            words: list - Liste des mots du document\n",
    "        \n",
    "        Returns:\n",
    "            list - Vecteur de fréquences pour chaque mot du vocabulaire\n",
    "        \"\"\"\n",
    "        # Créer un vecteur de zéros\n",
    "        bow_vector = [0] * self.vocab_size\n",
    "        \n",
    "        # Compter les occurrences\n",
    "        for word in words:\n",
    "            if word in self.vocabulary:\n",
    "                idx = self.vocabulary[word]\n",
    "                bow_vector[idx] += 1\n",
    "        \n",
    "        return bow_vector\n",
    "    \n",
    "    def encode_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Encoder plusieurs documents.\n",
    "        \n",
    "        Args:\n",
    "            documents: list - Liste de listes de mots\n",
    "        \n",
    "        Returns:\n",
    "            list - Liste de vecteurs BoW\n",
    "        \"\"\"\n",
    "        encoded_docs = []\n",
    "        for doc in documents:\n",
    "            encoded_docs.append(self.encode_document(doc))\n",
    "        return encoded_docs\n",
    "    \n",
    "    def get_word_frequencies(self, vector):\n",
    "        \"\"\"\n",
    "        Obtenir les fréquences de chaque mot à partir d'un vecteur BoW.\n",
    "        \n",
    "        Args:\n",
    "            vector: list - Vecteur BoW\n",
    "        \n",
    "        Returns:\n",
    "            dict - {mot: fréquence} pour les mots avec fréquence > 0\n",
    "        \"\"\"\n",
    "        frequencies = {}\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            if vector[idx] > 0:\n",
    "                frequencies[word] = vector[idx]\n",
    "        return frequencies\n",
    "    \n",
    "    def get_top_words(self, vector, top_n=10):\n",
    "        \"\"\"\n",
    "        Obtenir les top N mots les plus fréquents.\n",
    "        \n",
    "        Args:\n",
    "            vector: list - Vecteur BoW\n",
    "            top_n: int - Nombre de mots à retourner\n",
    "        \n",
    "        Returns:\n",
    "            list - Liste de tuples (mot, fréquence) triés par fréquence décroissante\n",
    "        \"\"\"\n",
    "        frequencies = self.get_word_frequencies(vector)\n",
    "        # Trier par fréquence décroissante\n",
    "        sorted_freq = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_freq[:top_n]\n",
    "    \n",
    "    def get_vocabulary_info(self):\n",
    "        \"\"\"Retourner les infos du vocabulaire\"\"\"\n",
    "        return {\n",
    "            'size': self.vocab_size,\n",
    "            'vocabulary': self.vocabulary\n",
    "        }\n",
    "\n",
    "\n",
    "# Tester BagOfWords\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST Bag of Words\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "if vocab:\n",
    "    # Créer le BoW\n",
    "    bow = BagOfWords(vocab)\n",
    "    \n",
    "    # Utiliser les tokens nettoyés comme un seul document\n",
    "    bow_vector = bow.encode_document(cleaned_tokens)\n",
    "    \n",
    "    print(f\"Document: {len(cleaned_tokens)} mots\")\n",
    "    print(f\"Vecteur BoW créé avec {bow.vocab_size} dimensions\\n\")\n",
    "    \n",
    "    # Obtenir les fréquences\n",
    "    frequencies = bow.get_word_frequencies(bow_vector)\n",
    "    print(f\"Nombre de mots uniques avec fréquence > 0: {len(frequencies)}\\n\")\n",
    "    \n",
    "    # Top 15 mots\n",
    "    print(\"Top 15 mots les plus fréquents:\")\n",
    "    print(\"─\" * 40)\n",
    "    top_words = bow.get_top_words(bow_vector, 15)\n",
    "    for word, freq in top_words:\n",
    "        pourcentage = (freq / sum(bow_vector)) * 100\n",
    "        barre = \"█\" * int(pourcentage / 2)\n",
    "        print(f\"{word:<15} | {freq:3d} | {pourcentage:5.2f}% {barre}\")\n",
    "    \n",
    "    # Statistiques\n",
    "    print(f\"\\n{'─' * 70}\")\n",
    "    print(f\"Vecteur BoW - Première moitié: {bow_vector[:10]}\")\n",
    "    print(f\"Nombre total de mots: {sum(bow_vector)}\")\n",
    "    print(f\"Moyenne d'occurrences par mot: {sum(bow_vector) / len(frequencies):.2f}\")\n",
    "    print(f\"Sparsité: {(len([x for x in bow_vector if x == 0]) / len(bow_vector) * 100):.2f}% de zéros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33e3e2",
   "metadata": {},
   "source": [
    "## 4. Comparaison OneHotEncoder vs Bag of Words\n",
    "\n",
    "Voir les différences entre les deux approches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a934f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARAISON OneHotEncoder vs Bag of Words\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "if vocab:\n",
    "    # Sélectionner 5 mots pour la démonstration\n",
    "    demo_words = vocab_sorted[10:15]\n",
    "    \n",
    "    print(f\"Mots de démonstration: {demo_words}\\n\")\n",
    "    \n",
    "    # === OneHotEncoder ===\n",
    "    print(\"OneHotEncoder - Représentation binaire:\")\n",
    "    print(\"─\" * 70)\n",
    "    print(\"Chaque mot a EXACTEMENT 1 seul 1, le reste 0\\n\")\n",
    "    \n",
    "    for word in demo_words:\n",
    "        vec = encoder.encode_word(word)\n",
    "        # Afficher le vecteur de manière compacte\n",
    "        display = [1 if v == 1 else 0 for v in vec]\n",
    "        print(f\"{word:<15} → Position {vec.index(1):4d} → Vecteur: [1 au rang {vec.index(1)}, 0 ailleurs]\")\n",
    "    \n",
    "    # === Bag of Words ===\n",
    "    print(\"\\n\\nBag of Words - Représentation par fréquences:\")\n",
    "    print(\"─\" * 70)\n",
    "    print(\"Chaque valeur représente le nombre d'occurrences du mot\\n\")\n",
    "    \n",
    "    # Créer un petit document avec répétitions\n",
    "    demo_document = demo_words + [demo_words[0], demo_words[1], demo_words[2]]\n",
    "    bow_vector = bow.encode_document(demo_document)\n",
    "    \n",
    "    print(f\"Document de test: {demo_document}\")\n",
    "    print(f\"Longueur: {len(demo_document)} mots\\n\")\n",
    "    \n",
    "    frequencies = bow.get_word_frequencies(bow_vector)\n",
    "    for word in demo_words:\n",
    "        freq = frequencies.get(word, 0)\n",
    "        barre = \"█\" * freq\n",
    "        print(f\"{word:<15} → Fréquence: {freq} {barre}\")\n",
    "    \n",
    "    # === Comparaison Tableau ===\n",
    "    print(\"\\n\\n\" + \"=\" * 70)\n",
    "    print(\"TABLEAU COMPARATIF\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    comparison = f\"\"\"\n",
    "┌─────────────────────┬──────────────────────┬──────────────────────┐\n",
    "│ CARACTÉRISTIQUE     │ OneHotEncoder        │ Bag of Words         │\n",
    "├─────────────────────┼──────────────────────┼──────────────────────┤\n",
    "│ Valeurs             │ 0 ou 1               │ Entiers (compte)     │\n",
    "│ 1 par vecteur       │ Oui (exactement 1)   │ Non (variable)       │\n",
    "│ Conserve l'ordre    │ Non                  │ Non                  │\n",
    "│ Préserve fréquences │ Non                  │ Oui                  │\n",
    "│ Sparsité            │ Très haute (~99.9%)  │ Très haute (~90%+)   │\n",
    "│ Dimension           │ Taille vocabulaire   │ Taille vocabulaire   │\n",
    "│ Cas d'usage         │ Classification       │ Recherche, BoW       │\n",
    "│ Mem/Calcul          │ Faible               │ Léger supérieur      │\n",
    "└─────────────────────┴──────────────────────┴──────────────────────┘\n",
    "\"\"\"\n",
    "    print(comparison)\n",
    "    \n",
    "    print(\"TAILLES MÉMOIRE COMPARÉES:\")\n",
    "    print(\"─\" * 70)\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    # OneHot: 1 vecteur = vocab_size bits\n",
    "    onehot_size = vocab_size / 8  # bits to bytes\n",
    "    \n",
    "    # BoW avec dense storage: vocab_size entiers (assume 4 bytes par entier)\n",
    "    bow_dense = (vocab_size * 4) / 1024  # to KB\n",
    "    \n",
    "    # BoW avec sparse storage: seulement les mots avec fréquence > 0\n",
    "    bow_sparse = (len(frequencies) * 8) / 1024  # 2 values per entry (word + freq)\n",
    "    \n",
    "    print(f\"Vocabulaire: {vocab_size} mots\")\n",
    "    print(f\"OneHotEncoder par mot: ~{onehot_size:.2f} bytes\")\n",
    "    print(f\"Bag of Words (dense): {bow_dense:.2f} KB par document\")\n",
    "    print(f\"Bag of Words (sparse): ~{bow_sparse:.2f} KB par document\")\n",
    "    print(f\"  → Gain mémoire (sparse): {(1 - bow_sparse/bow_dense)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8090f0",
   "metadata": {},
   "source": [
    "## 5. Résumé et Utilisation\n",
    "\n",
    "Guide pratique pour utiliser les deux encodeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f89dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RÉSUMÉ - PIPELINE COMPLET\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "summary = \"\"\"\n",
    "ÉTAPE 1: VOCABULAIRE (main.ipynb)\n",
    "├─ cleaned_tokens → Mots prétraités\n",
    "└─ vocab = set(cleaned_tokens) → Vocabulaire unique\n",
    "\n",
    "ÉTAPE 2: OneHotEncoder (main2.ipynb)\n",
    "├─ Entrée: Vocabulaire + Mot\n",
    "├─ Sortie: Vecteur [0,0,...,1,...,0] (n dimensions)\n",
    "├─ Utilisation: \n",
    "│  • Classification binaire\n",
    "│  • Encodage de catégories\n",
    "│  • Entrée de réseaux de neurones\n",
    "└─ Limite: Perd les informations de fréquence\n",
    "\n",
    "ÉTAPE 3: Bag of Words (main2.ipynb)\n",
    "├─ Entrée: Vocabulaire + Document (liste de mots)\n",
    "├─ Sortie: Vecteur [f1, f2, ..., fn] (fréquences)\n",
    "├─ Utilisation:\n",
    "│  • Analyse de sentiment\n",
    "│  • Recherche de similarité\n",
    "│  • Modèles probabilistes\n",
    "└─ Avantage: Conserve les fréquences\n",
    "\n",
    "VOCABULAIRE CRÉE:\n",
    "├─ Taille: \"\"\" + str(len(vocab)) + \"\"\" mots uniques\n",
    "├─ Utilisé par: OneHotEncoder et BagOfWords\n",
    "└─ Indexation: 0 à \"\"\" + str(len(vocab)-1) + \"\"\"\n",
    "\n",
    "DONNÉES:\n",
    "├─ Tokens nettoyés: \"\"\" + str(len(cleaned_tokens)) + \"\"\" mots\n",
    "├─ Tokens uniques: \"\"\" + str(len(vocab)) + \"\"\" mots\n",
    "└─ Densité: \"\"\" + f\"{(len(vocab)/len(cleaned_tokens)*100):.1f}%\" + \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Exemple d'utilisation complet\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXEMPLE D'UTILISATION COMPLET\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "if vocab:\n",
    "    # Diviser en phrases pour la démo\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    for token in cleaned_tokens[:100]:  # Utiliser les 100 premiers tokens\n",
    "        current_sentence.append(token)\n",
    "        if len(current_sentence) >= 8:  # Phrase de 8 mots\n",
    "            sentences.append(current_sentence)\n",
    "            current_sentence = []\n",
    "    \n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    \n",
    "    print(f\"Nombre de phrases créées: {len(sentences)}\\n\")\n",
    "    \n",
    "    # Encoder avec BoW\n",
    "    print(\"Encoding avec Bag of Words:\")\n",
    "    print(\"─\" * 70)\n",
    "    \n",
    "    bow_documents = bow.encode_documents(sentences[:3])  # Premières 3 phrases\n",
    "    \n",
    "    for i, (sent, bow_vec) in enumerate(zip(sentences[:3], bow_documents)):\n",
    "        print(f\"\\nPhrase {i+1}: {' '.join(sent)}\")\n",
    "        print(f\"Longueur: {len(sent)} mots\")\n",
    "        freq_dict = bow.get_word_frequencies(bow_vec)\n",
    "        print(f\"Mots uniques: {len(freq_dict)}\")\n",
    "        top = bow.get_top_words(bow_vec, 5)\n",
    "        print(f\"Top 5: {top}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ Vectorisation terminée!\")\n",
    "print(\"  • Vocabulaire créé ✓\")\n",
    "print(\"  • OneHotEncoder fonctionnel ✓\")\n",
    "print(\"  • Bag of Words fonctionnel ✓\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32689b29",
   "metadata": {},
   "source": [
    "## 6. Modèles Word Embeddings Avancés\n",
    "\n",
    "Implémentation de Continuous Bag of Words (CBOW), Skip-Gram, et approches distribuées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae3a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WORD EMBEDDINGS AVANCÉS\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONTINUOUS BAG OF WORDS (CBOW)\n",
    "# ============================================================================\n",
    "\n",
    "class ContinuousBagOfWords:\n",
    "    \"\"\"\n",
    "    Continuous Bag of Words (CBOW).\n",
    "    Prédit un mot target à partir du contexte environnant (mots avant/après).\n",
    "    \n",
    "    Exemple: \"Le [?] marche dans la rue\"\n",
    "    Contexte: [le, marche, dans] → Cible: chat\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary, window_size=2, embedding_dim=10):\n",
    "        \"\"\"\n",
    "        Initialiser CBOW.\n",
    "        \n",
    "        Args:\n",
    "            vocabulary: list - Vocabulaire unique\n",
    "            window_size: int - Nombre de mots de contexte de chaque côté\n",
    "            embedding_dim: int - Dimension des embeddings\n",
    "        \"\"\"\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialiser les embeddings aléatoirement\n",
    "        np.random.seed(42)\n",
    "        self.embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        print(f\"✓ CBOW initialisé\")\n",
    "        print(f\"  Vocabulaire: {self.vocab_size} mots\")\n",
    "        print(f\"  Dimension des embeddings: {embedding_dim}\")\n",
    "        print(f\"  Taille du contexte: ±{window_size} mots\\n\")\n",
    "    \n",
    "    def get_context_windows(self, words):\n",
    "        \"\"\"\n",
    "        Extraire les fenêtres de contexte d'une liste de mots.\n",
    "        \n",
    "        Args:\n",
    "            words: list - Séquence de mots\n",
    "        \n",
    "        Returns:\n",
    "            list - [(contexte, target), ...]\n",
    "        \"\"\"\n",
    "        windows = []\n",
    "        for i in range(len(words)):\n",
    "            # Déterminer les limites du contexte\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(words), i + self.window_size + 1)\n",
    "            \n",
    "            # Créer le contexte (sans le mot target)\n",
    "            context = []\n",
    "            for j in range(start, end):\n",
    "                if j != i and words[j] in self.vocabulary:\n",
    "                    context.append(words[j])\n",
    "            \n",
    "            # Ajouter si contexte non vide\n",
    "            if context and words[i] in self.vocabulary:\n",
    "                windows.append((context, words[i]))\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Obtenir l'embedding d'un mot\"\"\"\n",
    "        if word in self.vocabulary:\n",
    "            idx = self.vocabulary[word]\n",
    "            return self.embeddings[idx]\n",
    "        return None\n",
    "    \n",
    "    def get_context_vector(self, context_words):\n",
    "        \"\"\"Moyenne des embeddings du contexte\"\"\"\n",
    "        vectors = [self.get_embedding(w) for w in context_words if w in self.vocabulary]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        return np.zeros(self.embedding_dim)\n",
    "    \n",
    "    def predict_word_from_context(self, context_words, top_n=5):\n",
    "        \"\"\"Prédire le mot le plus probable selon le contexte\"\"\"\n",
    "        context_vec = self.get_context_vector(context_words)\n",
    "        \n",
    "        # Calculer la similarité avec tous les mots\n",
    "        similarities = []\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            sim = np.dot(context_vec, self.embeddings[idx])\n",
    "            similarities.append((word, sim))\n",
    "        \n",
    "        # Trier par similarité décroissante\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SKIP-GRAM\n",
    "# ============================================================================\n",
    "\n",
    "class SkipGram:\n",
    "    \"\"\"\n",
    "    Skip-Gram Model.\n",
    "    Prédit les mots de contexte à partir du mot target.\n",
    "    \n",
    "    Exemple: Chat → [le, marche, dans, ...]\n",
    "    Inverse de CBOW.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary, window_size=2, embedding_dim=10):\n",
    "        \"\"\"\n",
    "        Initialiser Skip-Gram.\n",
    "        \n",
    "        Args:\n",
    "            vocabulary: list - Vocabulaire unique\n",
    "            window_size: int - Nombre de mots de contexte de chaque côté\n",
    "            embedding_dim: int - Dimension des embeddings\n",
    "        \"\"\"\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialiser les embeddings\n",
    "        np.random.seed(42)\n",
    "        self.input_embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        self.output_embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        print(f\"✓ Skip-Gram initialisé\")\n",
    "        print(f\"  Vocabulaire: {self.vocab_size} mots\")\n",
    "        print(f\"  Dimension des embeddings: {embedding_dim}\")\n",
    "        print(f\"  Taille du contexte: ±{window_size} mots\\n\")\n",
    "    \n",
    "    def get_training_pairs(self, words):\n",
    "        \"\"\"\n",
    "        Extraire les paires (target, context) pour l'entraînement.\n",
    "        \n",
    "        Args:\n",
    "            words: list - Séquence de mots\n",
    "        \n",
    "        Returns:\n",
    "            list - [(target, contexte), ...]\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in self.vocabulary:\n",
    "                continue\n",
    "            \n",
    "            target = words[i]\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(words), i + self.window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if j != i and words[j] in self.vocabulary:\n",
    "                    pairs.append((target, words[j]))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"Obtenir l'embedding d'un mot (input)\"\"\"\n",
    "        if word in self.vocabulary:\n",
    "            idx = self.vocabulary[word]\n",
    "            return self.input_embeddings[idx]\n",
    "        return None\n",
    "    \n",
    "    def predict_context(self, target_word, top_n=5):\n",
    "        \"\"\"Prédire les mots de contexte probables\"\"\"\n",
    "        if target_word not in self.vocabulary:\n",
    "            return []\n",
    "        \n",
    "        target_idx = self.vocabulary[target_word]\n",
    "        target_vec = self.input_embeddings[target_idx]\n",
    "        \n",
    "        # Calculer les scores avec les embeddings de sortie\n",
    "        similarities = []\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            sim = np.dot(target_vec, self.output_embeddings[idx])\n",
    "            similarities.append((word, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DISTRIBUTED BAG OF WORDS (DBOW)\n",
    "# ============================================================================\n",
    "\n",
    "class DistributedBagOfWords:\n",
    "    \"\"\"\n",
    "    Distributed Bag of Words (Doc2Vec - PV-DBOW).\n",
    "    Représente chaque document par un vecteur continu.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary, embedding_dim=20):\n",
    "        \"\"\"\n",
    "        Initialiser Distributed BoW.\n",
    "        \n",
    "        Args:\n",
    "            vocabulary: list - Vocabulaire unique\n",
    "            embedding_dim: int - Dimension des embeddings\n",
    "        \"\"\"\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embeddings des mots et des documents\n",
    "        np.random.seed(42)\n",
    "        self.word_embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        print(f\"✓ Distributed Bag of Words initialisé\")\n",
    "        print(f\"  Vocabulaire: {self.vocab_size} mots\")\n",
    "        print(f\"  Dimension des embeddings: {embedding_dim}\\n\")\n",
    "    \n",
    "    def get_document_vector(self, words, doc_id=None):\n",
    "        \"\"\"\n",
    "        Créer un vecteur pour un document.\n",
    "        Moyenne des embeddings des mots + ID du document.\n",
    "        \n",
    "        Args:\n",
    "            words: list - Mots du document\n",
    "            doc_id: int - Identifiant unique du document\n",
    "        \n",
    "        Returns:\n",
    "            np.array - Vecteur du document\n",
    "        \"\"\"\n",
    "        # Moyenner les embeddings des mots\n",
    "        word_vecs = [self.word_embeddings[self.vocabulary[w]] \n",
    "                     for w in words if w in self.vocabulary]\n",
    "        \n",
    "        if word_vecs:\n",
    "            doc_vec = np.mean(word_vecs, axis=0)\n",
    "        else:\n",
    "            doc_vec = np.zeros(self.embedding_dim)\n",
    "        \n",
    "        return doc_vec\n",
    "    \n",
    "    def get_document_similarity(self, doc1_words, doc2_words):\n",
    "        \"\"\"Calculer la similarité cosinus entre deux documents\"\"\"\n",
    "        vec1 = self.get_document_vector(doc1_words)\n",
    "        vec2 = self.get_document_vector(doc2_words)\n",
    "        \n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm1 > 0 and norm2 > 0:\n",
    "            return np.dot(vec1, vec2) / (norm1 * norm2)\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. DISTRIBUTED MEMORY (DM)\n",
    "# ============================================================================\n",
    "\n",
    "class DistributedMemory:\n",
    "    \"\"\"\n",
    "    Distributed Memory (Doc2Vec - PV-DM).\n",
    "    Combine le contexte des mots ET l'ID du document pour prédire.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary, window_size=2, embedding_dim=20):\n",
    "        \"\"\"\n",
    "        Initialiser Distributed Memory.\n",
    "        \n",
    "        Args:\n",
    "            vocabulary: list - Vocabulaire unique\n",
    "            window_size: int - Taille du contexte\n",
    "            embedding_dim: int - Dimension des embeddings\n",
    "        \"\"\"\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Embeddings\n",
    "        np.random.seed(42)\n",
    "        self.word_embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        print(f\"✓ Distributed Memory initialisé\")\n",
    "        print(f\"  Vocabulaire: {self.vocab_size} mots\")\n",
    "        print(f\"  Dimension des embeddings: {embedding_dim}\")\n",
    "        print(f\"  Taille du contexte: ±{window_size} mots\\n\")\n",
    "    \n",
    "    def get_context_windows_with_doc(self, words, doc_id=0):\n",
    "        \"\"\"\n",
    "        Extraire les fenêtres incluant l'ID du document.\n",
    "        \n",
    "        Args:\n",
    "            words: list - Mots du document\n",
    "            doc_id: int - Identifiant du document\n",
    "        \n",
    "        Returns:\n",
    "            list - [(contexte + doc_id, target), ...]\n",
    "        \"\"\"\n",
    "        windows = []\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in self.vocabulary:\n",
    "                continue\n",
    "            \n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(words), i + self.window_size + 1)\n",
    "            \n",
    "            context = []\n",
    "            for j in range(start, end):\n",
    "                if j != i and words[j] in self.vocabulary:\n",
    "                    context.append(words[j])\n",
    "            \n",
    "            if context:\n",
    "                # Ajouter l'ID du document au contexte\n",
    "                context_with_doc = context + [f\"DOC_{doc_id}\"]\n",
    "                windows.append((context_with_doc, words[i]))\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def get_document_representation(self, words, doc_id=0):\n",
    "        \"\"\"\n",
    "        Obtenir une représentation du document combinant\n",
    "        les mots et l'ID du document.\n",
    "        \"\"\"\n",
    "        word_vecs = [self.word_embeddings[self.vocabulary[w]] \n",
    "                     for w in words if w in self.vocabulary]\n",
    "        \n",
    "        if word_vecs:\n",
    "            return np.mean(word_vecs, axis=0)\n",
    "        return np.zeros(self.embedding_dim)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTS ET COMPARAISONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TESTS DES MODÈLES\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "if vocab and data_loaded:\n",
    "    # Créer les modèles\n",
    "    cbow = ContinuousBagOfWords(vocab, window_size=2, embedding_dim=10)\n",
    "    skipgram = SkipGram(vocab, window_size=2, embedding_dim=10)\n",
    "    dbow = DistributedBagOfWords(vocab, embedding_dim=20)\n",
    "    dm = DistributedMemory(vocab, window_size=2, embedding_dim=20)\n",
    "    \n",
    "    # Utiliser les premiers tokens\n",
    "    sample_words = cleaned_tokens[:50]\n",
    "    \n",
    "    # === Test CBOW ===\n",
    "    print(\"─\" * 70)\n",
    "    print(\"1. CONTINUOUS BAG OF WORDS (CBOW)\")\n",
    "    print(\"─\" * 70)\n",
    "    print(\"Prédit un mot à partir du contexte environnant\\n\")\n",
    "    \n",
    "    cbow_windows = cbow.get_context_windows(sample_words)\n",
    "    print(f\"Fenêtres de contexte extraites: {len(cbow_windows)}\\n\")\n",
    "    \n",
    "    # Exemple\n",
    "    if cbow_windows:\n",
    "        context, target = cbow_windows[0]\n",
    "        print(f\"Exemple:\")\n",
    "        print(f\"  Contexte: {context}\")\n",
    "        print(f\"  Cible réelle: {target}\")\n",
    "        predictions = cbow.predict_word_from_context(context, top_n=5)\n",
    "        print(f\"  Top 5 prédictions:\")\n",
    "        for word, score in predictions:\n",
    "            print(f\"    {word:<15} score: {score:.4f}\")\n",
    "    \n",
    "    # === Test Skip-Gram ===\n",
    "    print(\"\\n\" + \"─\" * 70)\n",
    "    print(\"2. SKIP-GRAM\")\n",
    "    print(\"─\" * 70)\n",
    "    print(\"Prédit le contexte à partir du mot cible\\n\")\n",
    "    \n",
    "    skipgram_pairs = skipgram.get_training_pairs(sample_words)\n",
    "    print(f\"Paires (target, contexte) créées: {len(skipgram_pairs)}\\n\")\n",
    "    \n",
    "    # Exemple\n",
    "    if skipgram_pairs:\n",
    "        target, context = skipgram_pairs[0]\n",
    "        print(f\"Exemple:\")\n",
    "        print(f\"  Mot cible: {target}\")\n",
    "        print(f\"  Contexte réel: {context}\")\n",
    "        predictions = skipgram.predict_context(target, top_n=5)\n",
    "        print(f\"  Top 5 contextes prédits:\")\n",
    "        for word, score in predictions:\n",
    "            print(f\"    {word:<15} score: {score:.4f}\")\n",
    "    \n",
    "    # === Test Distributed Bag of Words ===\n",
    "    print(\"\\n\" + \"─\" * 70)\n",
    "    print(\"3. DISTRIBUTED BAG OF WORDS (DBOW)\")\n",
    "    print(\"─\" * 70)\n",
    "    print(\"Représente un document par un vecteur continu\\n\")\n",
    "    \n",
    "    # Diviser en documents\n",
    "    doc1 = cleaned_tokens[:30]\n",
    "    doc2 = cleaned_tokens[30:60]\n",
    "    doc3 = cleaned_tokens[60:90]\n",
    "    \n",
    "    vec1 = dbow.get_document_vector(doc1)\n",
    "    vec2 = dbow.get_document_vector(doc2)\n",
    "    vec3 = dbow.get_document_vector(doc3)\n",
    "    \n",
    "    print(f\"Vecteur document 1: {vec1[:5]}... (taille: {vec1.shape})\")\n",
    "    print(f\"Vecteur document 2: {vec2[:5]}... (taille: {vec2.shape})\")\n",
    "    print(f\"Vecteur document 3: {vec3[:5]}... (taille: {vec3.shape})\\n\")\n",
    "    \n",
    "    # Similarité entre documents\n",
    "    sim_1_2 = dbow.get_document_similarity(doc1, doc2)\n",
    "    sim_1_3 = dbow.get_document_similarity(doc1, doc3)\n",
    "    sim_2_3 = dbow.get_document_similarity(doc2, doc3)\n",
    "    \n",
    "    print(\"Similarités cosinus entre documents:\")\n",
    "    print(f\"  Doc1 vs Doc2: {sim_1_2:.4f}\")\n",
    "    print(f\"  Doc1 vs Doc3: {sim_1_3:.4f}\")\n",
    "    print(f\"  Doc2 vs Doc3: {sim_2_3:.4f}\")\n",
    "    \n",
    "    # === Test Distributed Memory ===\n",
    "    print(\"\\n\" + \"─\" * 70)\n",
    "    print(\"4. DISTRIBUTED MEMORY (DM)\")\n",
    "    print(\"─\" * 70)\n",
    "    print(\"Combine contexte ET ID du document pour prédire\\n\")\n",
    "    \n",
    "    dm_windows = dm.get_context_windows_with_doc(sample_words[:30], doc_id=1)\n",
    "    print(f\"Fenêtres (contexte + doc_id, target) créées: {len(dm_windows)}\\n\")\n",
    "    \n",
    "    if dm_windows:\n",
    "        context, target = dm_windows[0]\n",
    "        print(f\"Exemple:\")\n",
    "        print(f\"  Contexte + DocID: {context}\")\n",
    "        print(f\"  Cible: {target}\")\n",
    "    \n",
    "    doc_repr = dm.get_document_representation(sample_words[:30], doc_id=1)\n",
    "    print(f\"  Représentation du doc: {doc_repr[:5]}... (taille: {doc_repr.shape})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ Modèles Word Embeddings terminés!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76e28b",
   "metadata": {},
   "source": [
    "## 7. Comparaison des Modèles\n",
    "\n",
    "Tableau récapitulatif des différentes approches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c57d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPARAISON COMPLÈTE DES MODÈLES DE VECTORISATION\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "comparison_table = \"\"\"\n",
    "┌─────────────────────┬──────────────┬──────────────┬──────────────┬──────────────┐\n",
    "│ MODÈLE              │ OneHot/BoW   │ CBOW         │ Skip-Gram    │ DBOW/DM      │\n",
    "├─────────────────────┼──────────────┼──────────────┼──────────────┼──────────────┤\n",
    "│ Type                │ Comptage     │ Prédiction   │ Prédiction   │ Embedding    │\n",
    "│ Entrée              │ Mots         │ Contexte     │ Mot          │ Mots+Doc     │\n",
    "│ Sortie              │ Vecteur 0/1  │ Mot prédit   │ Contexte     │ Embedding    │\n",
    "│ Apprentissage       │ Non          │ Neuronal     │ Neuronal     │ Neuronal     │\n",
    "│ Dimension           │ |V| (sparse) │ embedding    │ embedding    │ embedding    │\n",
    "│ Sémantique          │ Non          │ Oui          │ Oui          │ Oui          │\n",
    "│ Similarité          │ Jaccard      │ Cosinus      │ Cosinus      │ Cosinus      │\n",
    "├─────────────────────┼──────────────┼──────────────┼──────────────┼──────────────┤\n",
    "│ Avantages           │ • Simple     │ • Rapide     │ • Flexible   │ • Document   │\n",
    "│                     │ • Sparse     │ • Efficace   │ • Détaillé   │ • Contexte   │\n",
    "│                     │ • Rapide     │ • Efficace   │ • Hiérarchie │ • Flexible   │\n",
    "├─────────────────────┼──────────────┼──────────────┼──────────────┼──────────────┤\n",
    "│ Inconvénients       │ • Pas séman. │ • Plus lent  │ • Plus lent  │ • Complexe   │\n",
    "│                     │ • Perte info │ • GPU besoin │ • GPU besoin │ • Training   │\n",
    "│                     │ • Haute dim. │              │              │              │\n",
    "├─────────────────────┼──────────────┼──────────────┼──────────────┼──────────────┤\n",
    "│ Cas d'usage         │ Recherche    │ Prédiction   │ Similarité   │ Classification│\n",
    "│                     │ Fil. contenu │ de contexte  │ de contexte  │ de docs      │\n",
    "│                     │ TF-IDF       │ Embedding    │ Embedding    │ Clustering   │\n",
    "├─────────────────────┼──────────────┼──────────────┼──────────────┼──────────────┤\n",
    "│ Complexité          │ O(|V|)       │ O(C×D)       │ O(C×D)       │ O(D×|V|)     │\n",
    "│                     │ Bas          │ Moyen        │ Moyen        │ Moyen        │\n",
    "├─────────────────────┼──────────────┼──────────────┼──────────────┼──────────────┤\n",
    "│ Équivalent          │ TF-IDF       │ Word2Vec     │ Word2Vec     │ Doc2Vec      │\n",
    "│ Réel                │ fastText     │ GloVe        │ FastText     │ Doc2Vec      │\n",
    "└─────────────────────┴──────────────┴──────────────┴──────────────┴──────────────┘\n",
    "\n",
    "LÉGENDE:\n",
    "  |V|     = Taille du vocabulaire\n",
    "  C       = Taille du contexte\n",
    "  D       = Dimension des embeddings\n",
    "  O()     = Complexité temporelle\n",
    "\"\"\"\n",
    "\n",
    "print(comparison_table)\n",
    "\n",
    "# Visualisation simple\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REPRÉSENTATION SCHÉMATIQUE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "schemas = \"\"\"\n",
    "1. CONTINUOUS BAG OF WORDS (CBOW)\n",
    "   ┌─────────┐  ┌─────────┐  ┌─────────┐\n",
    "   │  \"Le\"   │  │ \"chat\"  │  │  \"dort\" │\n",
    "   └────┬────┘  └────┬────┘  └────┬────┘\n",
    "        │            │            │\n",
    "        └─────────────┼─────────────┘\n",
    "                      ↓\n",
    "              [Réseau de Neurones]\n",
    "                      ↓\n",
    "              Prédire: \"marche\"\n",
    "              \n",
    "2. SKIP-GRAM\n",
    "   ┌─────────┐\n",
    "   │ \"chat\"  │ ← Mot cible\n",
    "   └────┬────┘\n",
    "        ↓\n",
    "   [Réseau de Neurones]\n",
    "        ↓\n",
    "   ┌────┴─────┬──────┬──────┐\n",
    "   ↓          ↓      ↓      ↓\n",
    "  \"le\"      \"dort\" \"dans\" \"rue\"\n",
    "  (contexte prédit)\n",
    "  \n",
    "3. DISTRIBUTED BAG OF WORDS (DBOW)\n",
    "   Document_ID: [1]\n",
    "   Mots: [chat, marche, rue]\n",
    "        ┌─────┬──────┬────┐\n",
    "        ↓     ↓      ↓    ↓\n",
    "   [Réseau avec Document_ID]\n",
    "        ↓\n",
    "   Vecteur du document: [0.2, -0.1, 0.5, ...]\n",
    "   \n",
    "4. DISTRIBUTED MEMORY (DM)\n",
    "   Document_ID + Contexte: [DOC_1, \"le\", \"chat\"]\n",
    "        ┌──────┬────┬────┐\n",
    "        ↓      ↓    ↓    ↓\n",
    "   [Réseau de Neurones]\n",
    "        ↓\n",
    "   Prédire mot suivant + apprendre vecteur du doc\n",
    "\"\"\"\n",
    "\n",
    "print(schemas)\n",
    "\n",
    "# Résumé statistique\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RÉSUMÉ STATISTIQUE\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "if vocab and data_loaded:\n",
    "    summary_stats = f\"\"\"\n",
    "DONNÉES DISPONIBLES:\n",
    "  • Vocabulaire: {len(vocab)} mots uniques\n",
    "  • Tokens nettoyés: {len(cleaned_tokens)} mots\n",
    "  • Densité: {(len(vocab)/len(cleaned_tokens)*100):.2f}%\n",
    "\n",
    "MODÈLES IMPLÉMENTÉS:\n",
    "  1. OneHot Encoder\n",
    "     - Dimensions: {len(vocab)}\n",
    "     - Sparsité: ~99.9%\n",
    "     \n",
    "  2. Bag of Words\n",
    "     - Dimensions: {len(vocab)}\n",
    "     - Représentation: fréquences\n",
    "     \n",
    "  3. Continuous Bag of Words (CBOW)\n",
    "     - Contexte: ±2 mots\n",
    "     - Embedding: 10 dimensions\n",
    "     - Fenêtres: ~{len(cbow_windows)} paires\n",
    "     \n",
    "  4. Skip-Gram\n",
    "     - Contexte: ±2 mots\n",
    "     - Embedding: 10 dimensions\n",
    "     - Paires training: ~{len(skipgram_pairs)} pairs\n",
    "     \n",
    "  5. Distributed Bag of Words (DBOW)\n",
    "     - Embedding: 20 dimensions\n",
    "     - Docs testés: 3\n",
    "     \n",
    "  6. Distributed Memory (DM)\n",
    "     - Contexte: ±2 mots\n",
    "     - Embedding: 20 dimensions\n",
    "     - Avec Doc ID: Oui\n",
    "\n",
    "PROCHAINES ÉTAPES:\n",
    "  ✓ Visualiser les embeddings (t-SNE, PCA)\n",
    "  ✓ Entraîner les modèles avec gradient descent\n",
    "  ✓ Évaluer la qualité des embeddings\n",
    "  ✓ Comparaison avec Word2Vec réel\n",
    "\"\"\"\n",
    "    print(summary_stats)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ Section Word Embeddings Avancés terminée!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

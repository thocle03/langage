{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4616707e",
   "metadata": {},
   "source": [
    "# NLP - Prétraitement de Texte\n",
    "## Cours NLP - Techniques essentielles de traitement de texte\n",
    "\n",
    "Ce notebook couvre les techniques fondamentales du prétraitement de texte en NLP avec des fonctions réutilisables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0aa10",
   "metadata": {},
   "source": [
    "## 1. Chargement et Lecture du Texte\n",
    "\n",
    "Fonction réutilisable pour charger des fichiers texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8976f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['nltk', 'spacy', 'unidecode', 'numpy', 'pandas']\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "print(\"✓ Bibliothèques installées avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee270f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from unidecode import unidecode\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Téléchargement des ressources NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Chargement du modèle spaCy français\n",
    "try:\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "except OSError:\n",
    "    print(\"Téléchargement du modèle spaCy français...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'spacy', 'download', 'fr_core_news_sm'])\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "print(\"✓ Imports et modèles chargés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b82878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_file(file_path):\n",
    "    \"\"\"\n",
    "    Charge un fichier texte et retourne son contenu.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Chemin vers le fichier texte\n",
    "    \n",
    "    Returns:\n",
    "        str: Contenu du fichier\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur: Le fichier {file_path} n'existe pas\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Charger le texte d'entrée\n",
    "text = load_text_file('entree.txt')\n",
    "print(f\"Texte chargé: {len(text)} caractères\")\n",
    "print(\"\\n--- Premier 200 caractères ---\")\n",
    "print(text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae72e78c",
   "metadata": {},
   "source": [
    "## 2. Tokenisation en Mots\n",
    "\n",
    "Fonction pour tokeniser le texte en mots individuels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef786a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Tokenise un texte en mots individuels.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte à tokeniser\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des mots\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    return tokens\n",
    "\n",
    "# Tester la tokenisation en mots\n",
    "words = tokenize_words(text)\n",
    "print(f\"Nombre de mots: {len(words)}\")\n",
    "print(f\"Premiers 20 mots: {words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dac8fc",
   "metadata": {},
   "source": [
    "## 3. Tokenisation en Phrases\n",
    "\n",
    "Fonction pour tokeniser le texte en phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14529175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    \"\"\"\n",
    "    Tokenise un texte en phrases.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte à tokeniser\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des phrases\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text, language='french')\n",
    "    return sentences\n",
    "\n",
    "# Tester la tokenisation en phrases\n",
    "sentences = tokenize_sentences(text)\n",
    "print(f\"Nombre de phrases: {len(sentences)}\")\n",
    "print(\"\\n--- Premières 3 phrases ---\")\n",
    "for i, sent in enumerate(sentences[:3], 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ac695",
   "metadata": {},
   "source": [
    "## 4. Génération de Bigrammes\n",
    "\n",
    "Fonction pour transformer une liste de mots en bigrammes (paires de mots consécutifs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe898be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(words):\n",
    "    \"\"\"\n",
    "    Génère des bigrammes à partir d'une liste de mots.\n",
    "    Un bigramme est une paire de deux mots consécutifs.\n",
    "    \n",
    "    Args:\n",
    "        words (list): Liste des mots\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des bigrammes (tuples de 2 mots)\n",
    "    \"\"\"\n",
    "    bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "    return bigrams\n",
    "\n",
    "# Tester la génération de bigrammes\n",
    "sample_words = words[:10]\n",
    "bigrams = generate_bigrams(sample_words)\n",
    "print(f\"Mots: {sample_words}\")\n",
    "print(f\"\\nBigrammes générés: {bigrams}\")\n",
    "print(f\"\\nNombre total de bigrammes dans le texte: {len(generate_bigrams(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba497f",
   "metadata": {},
   "source": [
    "## 5. Normalisation Unicode\n",
    "\n",
    "Fonction complète de normalisation: minuscules, suppression accents, normalisation des nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalise le texte:\n",
    "    - Conversion en minuscules\n",
    "    - Suppression des accents\n",
    "    - Normalisation des nombres\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte à normaliser\n",
    "    \n",
    "    Returns:\n",
    "        str: Texte normalisé\n",
    "    \"\"\"\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer les accents avec unidecode\n",
    "    text = unidecode(text)\n",
    "    \n",
    "    # Normalisation des nombres (optionnel: remplacer par <NUM>)\n",
    "    # Décommentez la ligne suivante pour remplacer les nombres par <NUM>\n",
    "    # text = re.sub(r'\\d+', '<NUM>', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Tester la normalisation\n",
    "sample_text = \"Noël est le 25 décembre. C'est une fêtE avec Événements !\"\n",
    "normalized = normalize_text(sample_text)\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Normalisé: {normalized}\")\n",
    "\n",
    "# Appliquer sur le texte complet\n",
    "normalized_text = normalize_text(text)\n",
    "print(f\"\\n✓ Texte entier normalisé ({len(normalized_text)} caractères)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c399bd",
   "metadata": {},
   "source": [
    "## 6. Suppression de Ponctuation et Stop Words\n",
    "\n",
    "Fonction pour supprimer la ponctuation et les mots vides (stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les stop words français\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "\n",
    "def remove_punctuation_and_stopwords(words, remove_stopwords=True):\n",
    "    \"\"\"\n",
    "    Supprime la ponctuation et optionnellement les stop words.\n",
    "    \n",
    "    Args:\n",
    "        words (list): Liste des mots (tokens)\n",
    "        remove_stopwords (bool): Si True, supprime les stop words\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des mots filtrés\n",
    "    \"\"\"\n",
    "    # Supprimer la ponctuation\n",
    "    filtered = [word for word in words if word not in string.punctuation]\n",
    "    \n",
    "    # Supprimer les stop words\n",
    "    if remove_stopwords:\n",
    "        filtered = [word for word in filtered if word.lower() not in french_stopwords]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Tester sur les premiers mots\n",
    "sample_words_raw = tokenize_words(\"Bonjour, ceci est un test ! Les stop words doivent être supprimés.\")\n",
    "print(f\"Avant filtrage: {sample_words_raw}\")\n",
    "filtered_words = remove_punctuation_and_stopwords(sample_words_raw)\n",
    "print(f\"\\nAprès filtrage: {filtered_words}\")\n",
    "\n",
    "# Appliquer sur le texte complet\n",
    "normalized_and_tokenized = tokenize_words(normalized_text)\n",
    "clean_words = remove_punctuation_and_stopwords(normalized_and_tokenized)\n",
    "print(f\"\\nTexte complet:\")\n",
    "print(f\"Mots avec stop words et ponctuation: {len(normalized_and_tokenized)}\")\n",
    "print(f\"Mots filtrés: {len(clean_words)}\")\n",
    "print(f\"Premiers mots filtrés: {clean_words[:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e985eb",
   "metadata": {},
   "source": [
    "## 7. Racinisation (Stemming)\n",
    "\n",
    "Fonction de stemming pour réduire les mots à leur racine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37918412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser le stemmer français (Snowball)\n",
    "stemmer = SnowballStemmer('french')\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"\n",
    "    Applique le stemming (racinisation) à une liste de mots.\n",
    "    Réduit les mots à leur racine/radical.\n",
    "    \n",
    "    Args:\n",
    "        words (list): Liste des mots\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des mots racinisés\n",
    "    \"\"\"\n",
    "    stemmed = [stemmer.stem(word) for word in words]\n",
    "    return stemmed\n",
    "\n",
    "# Tester le stemming\n",
    "test_words = ['courant', 'courants', 'courant', 'couru', 'courserait', \n",
    "              'clair', 'clarté', 'clairement', 'briller', 'brillant']\n",
    "stemmed = stem_words(test_words)\n",
    "\n",
    "print(\"Stemming - Réduction à la racine:\")\n",
    "print(\"-\" * 50)\n",
    "for original, stem in zip(test_words, stemmed):\n",
    "    print(f\"{original:15} → {stem}\")\n",
    "\n",
    "# Appliquer sur les mots nettoyés\n",
    "stemmed_words = stem_words(clean_words)\n",
    "print(f\"\\n✓ {len(stemmed_words)} mots racinisés\")\n",
    "print(f\"Exemple: {clean_words[:5]} → {stemmed_words[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d82478",
   "metadata": {},
   "source": [
    "## 8. Lemmatisation\n",
    "\n",
    "Fonction de lemmatisation pour ramener les mots à leur forme canonique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f43f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Applique la lemmatisation au texte en utilisant spaCy.\n",
    "    Ramène chaque mot à sa forme canonique (lemme).\n",
    "    \n",
    "    Args:\n",
    "        text (str): Texte à lemmatiser\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des lemmes\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "\n",
    "# Tester la lemmatisation\n",
    "test_text = \"courant courants couru courserait clair clarté clairement briller brillant\"\n",
    "lemmas = lemmatize_text(test_text)\n",
    "\n",
    "print(\"Lemmatisation avec spaCy:\")\n",
    "print(\"-\" * 50)\n",
    "doc = nlp(test_text)\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} → {token.lemma_:15} ({token.pos_})\")\n",
    "\n",
    "# Appliquer sur le texte complet\n",
    "lemmas_full = lemmatize_text(normalized_text)\n",
    "print(f\"\\n✓ {len(lemmas_full)} lemmes générés\")\n",
    "print(f\"Premiers lemmes: {lemmas_full[:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc05823",
   "metadata": {},
   "source": [
    "## 9. Pipeline Complet de Prétraitement\n",
    "\n",
    "Exemple d'utilisation du pipeline complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Classe réutilisable pour le prétraitement de texte (style Java).\n",
    "    Permet de chaîner les opérations de nettoyage et de traitement.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.original_text = None\n",
    "        self.processed_text = None\n",
    "        self.tokens = None\n",
    "        self.sentences = None\n",
    "    \n",
    "    def load_file(self, file_path):\n",
    "        \"\"\"Charge un fichier texte\"\"\"\n",
    "        self.original_text = load_text_file(file_path)\n",
    "        return self\n",
    "    \n",
    "    def set_text(self, text):\n",
    "        \"\"\"Définit le texte directement\"\"\"\n",
    "        self.original_text = text\n",
    "        self.processed_text = text\n",
    "        return self\n",
    "    \n",
    "    def normalize(self):\n",
    "        \"\"\"Applique la normalisation Unicode\"\"\"\n",
    "        if self.processed_text is None:\n",
    "            self.processed_text = self.original_text\n",
    "        self.processed_text = normalize_text(self.processed_text)\n",
    "        return self\n",
    "    \n",
    "    def tokenize_into_words(self):\n",
    "        \"\"\"Tokenise en mots\"\"\"\n",
    "        text = self.processed_text if self.processed_text else self.original_text\n",
    "        self.tokens = tokenize_words(text)\n",
    "        return self\n",
    "    \n",
    "    def tokenize_into_sentences(self):\n",
    "        \"\"\"Tokenise en phrases\"\"\"\n",
    "        text = self.original_text if self.original_text else self.processed_text\n",
    "        self.sentences = tokenize_sentences(text)\n",
    "        return self\n",
    "    \n",
    "    def clean_tokens(self, remove_stopwords=True):\n",
    "        \"\"\"Supprime ponctuation et stop words\"\"\"\n",
    "        if self.tokens is None:\n",
    "            self.tokenize_into_words()\n",
    "        self.tokens = remove_punctuation_and_stopwords(self.tokens, remove_stopwords)\n",
    "        return self\n",
    "    \n",
    "    def stem(self):\n",
    "        \"\"\"Applique le stemming\"\"\"\n",
    "        if self.tokens is None:\n",
    "            self.tokenize_into_words()\n",
    "        self.tokens = stem_words(self.tokens)\n",
    "        return self\n",
    "    \n",
    "    def lemmatize(self):\n",
    "        \"\"\"Applique la lemmatisation\"\"\"\n",
    "        text = self.processed_text if self.processed_text else self.original_text\n",
    "        lemmas = lemmatize_text(text)\n",
    "        self.tokens = lemmas\n",
    "        return self\n",
    "    \n",
    "    def get_bigrams(self):\n",
    "        \"\"\"Génère les bigrammes\"\"\"\n",
    "        if self.tokens is None:\n",
    "            self.tokenize_into_words()\n",
    "        return generate_bigrams(self.tokens)\n",
    "    \n",
    "    def get_tokens(self):\n",
    "        \"\"\"Retourne les tokens actuels\"\"\"\n",
    "        return self.tokens\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        \"\"\"Retourne les phrases\"\"\"\n",
    "        return self.sentences\n",
    "    \n",
    "    def get_text(self):\n",
    "        \"\"\"Retourne le texte traité\"\"\"\n",
    "        return self.processed_text if self.processed_text else self.original_text\n",
    "\n",
    "\n",
    "# Tester la classe\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE COMPLET - Style Java avec Builder Pattern\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Créer un préprocesseur et appliquer les transformations\n",
    "processor = TextPreprocessor()\n",
    "processor.load_file('entree.txt').normalize().tokenize_into_words().clean_tokens()\n",
    "\n",
    "print(f\"\\n✓ Texte chargé, normalisé et tokenisé\")\n",
    "print(f\"  Nombre de tokens: {len(processor.get_tokens())}\")\n",
    "print(f\"  Premiers tokens: {processor.get_tokens()[:10]}\")\n",
    "\n",
    "# Générer des bigrammes\n",
    "bigrams = processor.get_bigrams()\n",
    "print(f\"\\n✓ Bigrammes générés: {len(bigrams)}\")\n",
    "print(f\"  Exemples: {bigrams[:5]}\")\n",
    "\n",
    "# Appliquer le stemming\n",
    "processor.stem()\n",
    "print(f\"\\n✓ Stemming appliqué\")\n",
    "print(f\"  Tokens après stemming: {processor.get_tokens()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e689b9d",
   "metadata": {},
   "source": [
    "## 10. Comparaison des Approches\n",
    "\n",
    "Comparaison entre stemming et lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ba58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Texte d'exemple\n",
    "example_text = \"Les restaurants servent à manger des mets délicieux. Manger est une action naturelle pour les mangeurs.\"\n",
    "\n",
    "print(\"COMPARAISON: STEMMING vs LEMMATISATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Texte original: {example_text}\\n\")\n",
    "\n",
    "# Tokenisation\n",
    "words = tokenize_words(example_text)\n",
    "normalized_words = normalize_text(example_text)\n",
    "tokens = tokenize_words(normalized_words)\n",
    "\n",
    "# Stemming\n",
    "stemmed = stem_words(tokens)\n",
    "\n",
    "# Lemmatisation\n",
    "lemmas = lemmatize_text(example_text)\n",
    "\n",
    "# Créer un tableau de comparaison\n",
    "data = {\n",
    "    'Mot Original': tokens,\n",
    "    'Stemming': stemmed,\n",
    "    'Lemmatisation': lemmas\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"• STEMMING: Coupe les mots de manière algorithmique (plus agressif)\")\n",
    "print(\"  Exemple: 'mangeurs' → 'mang'\")\n",
    "print(\"\\n• LEMMATISATION: Ramène à la forme de base grammaticale (plus linguistique)\")\n",
    "print(\"  Exemple: 'mangeurs' → 'mangeur'\")\n",
    "print(\"\\nSTEMMING est plus rapide mais peut être trop agressif\")\n",
    "print(\"LEMMATISATION est plus précis mais requiert plus de ressources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7d5c2",
   "metadata": {},
   "source": [
    "## 11. Résumé des Fonctions\n",
    "\n",
    "Guide d'utilisation des fonctions principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1ad58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    RÉSUMÉ DES FONCTIONS DISPONIBLES                        ║\n",
    "╚════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "1. CHARGEMENT DE TEXTE\n",
    "   └─ load_text_file(file_path)\n",
    "      Charge un fichier texte et retourne son contenu\n",
    "\n",
    "2. TOKENISATION\n",
    "   ├─ tokenize_words(text)\n",
    "   │  Divise le texte en mots individuels\n",
    "   └─ tokenize_sentences(text)\n",
    "      Divise le texte en phrases\n",
    "\n",
    "3. BIGRAMMES\n",
    "   └─ generate_bigrams(words)\n",
    "      Crée des paires de mots consécutifs\n",
    "\n",
    "4. NORMALISATION UNICODE\n",
    "   └─ normalize_text(text)\n",
    "      - Convertit en minuscules\n",
    "      - Supprime les accents\n",
    "      - Normalise les nombres\n",
    "\n",
    "5. NETTOYAGE\n",
    "   └─ remove_punctuation_and_stopwords(words, remove_stopwords=True)\n",
    "      Supprime la ponctuation et les mots vides\n",
    "\n",
    "6. RACINISATION\n",
    "   └─ stem_words(words)\n",
    "      Réduit les mots à leur racine (algorithme Snowball)\n",
    "\n",
    "7. LEMMATISATION\n",
    "   └─ lemmatize_text(text)\n",
    "      Ramène les mots à leur forme canonique (spaCy)\n",
    "\n",
    "8. CLASSE RÉUTILISABLE\n",
    "   └─ TextPreprocessor\n",
    "      Classe avec méthodes chaînables (Builder Pattern):\n",
    "      - load_file(path)\n",
    "      - set_text(text)\n",
    "      - normalize()\n",
    "      - tokenize_into_words()\n",
    "      - tokenize_into_sentences()\n",
    "      - clean_tokens(remove_stopwords=True)\n",
    "      - stem()\n",
    "      - lemmatize()\n",
    "      - get_bigrams()\n",
    "      - get_tokens()\n",
    "      - get_sentences()\n",
    "      - get_text()\n",
    "\n",
    "╔════════════════════════════════════════════════════════════════════════════╗\n",
    "║                         EXEMPLES D'UTILISATION                             ║\n",
    "╚════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")\n",
    "\n",
    "# EXEMPLE 1: Utilisation simple\n",
    "print(\"\\\\n[EXEMPLE 1] - Utilisation simple\")\n",
    "print(\"-\" * 80)\n",
    "text1 = \"Bonjour, ceci est un texte d'exemple avec des accents: café, élève, français !\"\n",
    "print(f\"Original: {text1}\")\n",
    "print(f\"Normalisé: {normalize_text(text1)}\")\n",
    "\n",
    "# EXEMPLE 2: Pipeline complet avec la classe\n",
    "print(\"\\\\n[EXEMPLE 2] - Pipeline complet avec TextPreprocessor\")\n",
    "print(\"-\" * 80)\n",
    "processor2 = TextPreprocessor()\n",
    "result2 = (processor2\n",
    "    .set_text(\"Les enfants jouent dans le parc. Jouer est amusant!\")\n",
    "    .normalize()\n",
    "    .tokenize_into_words()\n",
    "    .clean_tokens()\n",
    ")\n",
    "print(f\"Tokens filtrés: {result2.get_tokens()}\")\n",
    "\n",
    "# EXEMPLE 3: Comparaison stemming vs lemmatisation\n",
    "print(\"\\\\n[EXEMPLE 3] - Stemming vs Lemmatisation\")\n",
    "print(\"-\" * 80)\n",
    "test_text3 = \"marcher marcheurs marche marchant\"\n",
    "words3 = tokenize_words(test_text3)\n",
    "print(f\"Original:       {words3}\")\n",
    "print(f\"Stemming:       {stem_words(words3)}\")\n",
    "print(f\"Lemmatisation:  {lemmatize_text(test_text3)}\")\n",
    "\n",
    "# EXEMPLE 4: Bigrammes\n",
    "print(\"\\\\n[EXEMPLE 4] - Génération de bigrammes\")\n",
    "print(\"-\" * 80)\n",
    "text4 = \"Python est un langage de programmation\"\n",
    "words4 = tokenize_words(normalize_text(text4))\n",
    "bigrams4 = generate_bigrams(words4)\n",
    "print(f\"Mots: {words4}\")\n",
    "print(f\"Bigrammes: {bigrams4}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"✓ Notebook NLP complet prêt à l'utilisation!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

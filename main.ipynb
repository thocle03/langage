{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01ee15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bibliothèques installées avec succès\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['nltk', 'spacy', 'unidecode', 'numpy', 'pandas']\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "print(\"Bibliothèques installées avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00a803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du modèle spaCy français...\n",
      "Imports et modèles chargés\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "except OSError:\n",
    "    print(\"Téléchargement du modèle spaCy français...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'spacy', 'download', 'fr_core_news_sm'])\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "print(\"Imports et modèles chargés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c67333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte chargé: 1616 caractères\n",
      "\n",
      "--- Premier 200 caractères ---\n",
      "OH OH OH, Noel arrive, et qui dit Noel dit ... nourriture !\n",
      "Jeudi 18 décembre prochain, à partir de midi, nous vous proposons un grand buffet collectif de noel pour fêter ensemble vos départs en vacan...\n"
     ]
    }
   ],
   "source": [
    "def load_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur: Le fichier {file_path} n'existe pas\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "text = load_text_file('entree.txt')\n",
    "print(f\"Texte chargé: {len(text)} caractères\")\n",
    "print(\"\\n--- Premier 200 caractères ---\")\n",
    "print(text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1213db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ressource NLTK manquante\n",
      "Nombre de mots: 316\n",
      "Premiers 20 mots: ['OH', 'OH', 'OH', ',', 'Noel', 'arrive', ',', 'et', 'qui', 'dit', 'Noel', 'dit', '.', '.', '.', 'nourriture', '!', 'Jeudi', '18', 'décembre']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_words(text):\n",
    "    print(f\"Ressource NLTK manquante\")\n",
    "    import re\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[.,!?;:\\-]', text)\n",
    "    return tokens\n",
    "\n",
    "words = tokenize_words(text)\n",
    "print(f\"Nombre de mots: {len(words)}\")\n",
    "print(f\"Premiers 20 mots: {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b92f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases: 13\n",
      "\n",
      "--- Premières 3 phrases ---\n",
      "1. OH OH OH, Noel arrive, et qui dit Noel dit \n",
      "2.  nourriture \n",
      "3. Jeudi 18 décembre prochain, à partir de midi, nous vous proposons un grand buffet collectif de noel pour fêter ensemble vos départs en vacances d'hiver après un premier semestre intense \n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(text):\n",
    "    sentences = re.split(r'\\.\\.\\.|[.!?]\\s*', text)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "sentences = tokenize_sentences(text)\n",
    "print(f\"Nombre de phrases: {len(sentences)}\")\n",
    "print(\"\\n--- Premières 3 phrases ---\")\n",
    "for i, sent in enumerate(sentences[:3], 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0515f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de bigrammes/trigrammes (séparés par -): 6\n",
      "les 10 premiers bi/tri grams: [('arc', 'en', 'ciel'), ('New', 'York'), ('wagon', 'restaurant'), ('chou', 'fleur'), ('arc', 'e'), ('n', 'ciel')]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_bigrams(text):\n",
    "    ngrams = []\n",
    "    trigrammes = re.findall(r'(\\w+)-(\\w+)-(\\w+)', text)\n",
    "    for tri in trigrammes:\n",
    "        ngrams.append(tri)  \n",
    "    \n",
    "    bigrammes = re.findall(r'(\\w+)-(\\w+)(?!-)', text)\n",
    "    for bi in bigrammes:\n",
    "        ngrams.append(bi) \n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "bigrams = tokenize_bigrams(text)\n",
    "print(f\"Nombre de bigrammes/trigrammes (séparés par -): {len(bigrams)}\")\n",
    "\n",
    "print (f\"les 10 premiers bi/tri grammes: {bigrams[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c93f9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST \n",
      "\n",
      "Original:  Noël est le 25 décembre! C'est une fêtE avec Événements.\n",
      "Normalisé: noel est le deux cinq  decembre! c'est une fete avec evenements.\n",
      "Nombre de caractères: 1616 → 1634\n",
      "\n",
      "Premier 200 caractères normalisés:\n",
      "oh oh oh, noel arrive, et qui dit noel dit ... nourriture !\n",
      "jeudi un huit  decembre prochain, a partir de midi, nous vous proposons un grand buffet collectif de noel pour feter ensemble vos departs en...\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    import unicodedata\n",
    "    text = text.lower()\n",
    "    \n",
    "    text_nfd = unicodedata.normalize('NFD', text)\n",
    "    text_without_accents = ''.join(char for char in text_nfd \n",
    "                                    if unicodedata.category(char) != 'Mn')\n",
    "    nombres_texte = {\n",
    "        '0': 'zero', '1': 'un', '2': 'deux', '3': 'trois', '4': 'quatre',\n",
    "        '5': 'cinq', '6': 'six', '7': 'sept', '8': 'huit', '9': 'neuf'\n",
    "    }\n",
    "    \n",
    "    text_normalized = ''\n",
    "    for char in text_without_accents:\n",
    "        if char.isdigit():\n",
    "            text_normalized += nombres_texte[char] + ' '\n",
    "        else:\n",
    "            text_normalized += char\n",
    "    \n",
    "    return text_normalized.strip()\n",
    "\n",
    "print(\"TEST \\n\")\n",
    "\n",
    "sample_text = \"Noël est le 25 décembre! C'est une fêtE avec Événements.\"\n",
    "normalized = normalize_text(sample_text)\n",
    "\n",
    "print(f\"Original:  {sample_text}\")\n",
    "print(f\"Normalisé: {normalized}\")\n",
    "\n",
    "text_normalized = normalize_text(text)\n",
    "print(f\"Nombre de caractères: {len(text)} → {len(text_normalized)}\")\n",
    "print(f\"\\nPremier 200 caractères normalisés:\")\n",
    "print(text_normalized[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98708590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netteoyage\n",
      "\n",
      "Ressource NLTK manquante\n",
      "Tokens avant nettoyage: 319\n",
      "Premiers 20 tokens: ['oh', 'oh', 'oh', ',', 'noel', 'arrive', ',', 'et', 'qui', 'dit', 'noel', 'dit', '.', '.', '.', 'nourriture', '!', 'jeudi', 'un', 'huit']\n",
      "\n",
      "Tokens après nettoyage: 156\n",
      "Premiers 20 tokens nettoyés: ['noel', 'arrive', 'dit', 'noel', 'dit', 'nourriture', 'jeudi', 'huit', 'decembre', 'prochain', 'partir', 'midi', 'proposons', 'grand', 'buffet', 'collectif', 'noel', 'feter', 'ensemble', 'departs']\n",
      "\n",
      "STATS\n",
      "Tokens supprimés: 163\n",
      "Ratio de compression: 51.1%\n"
     ]
    }
   ],
   "source": [
    "french_stopwords = {\n",
    "    'le', 'la', 'les', 'de', 'des', 'un', 'une', 'et', 'ou', 'mais', 'donc',\n",
    "    'car', 'par', 'pour', 'avec', 'sans', 'sur', 'sous', 'dans', 'entre',\n",
    "    'avant', 'apres', 'pendant', 'lors', 'qui', 'que', 'quoi', 'quel', 'quelle',\n",
    "    'dont', 'duquel', 'auquel', 'a', 'au', 'aux', 'ce', 'cet', 'cette', 'ces',\n",
    "    'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles', 'on', 'mon', 'ton',\n",
    "    'son', 'notre', 'votre', 'leur', 'mes', 'tes', 'ses', 'nos', 'vos', 'leurs',\n",
    "    'moi', 'toi', 'lui', 'me', 'te', 'se', 'nous', 'vous', 'leur', 'est', 'etre',\n",
    "    'avoir', 'avoir', 'faire', 'aller', 'venir', 'pouvoir', 'devoir', 'vouloir',\n",
    "    'c', 'est', 'd', 'l', 'j', 's', 't', 'n', 'm', 'y', 'en', 'a', 'as', 'oh'\n",
    "}\n",
    "\n",
    "def clean_tokens(tokens, remove_stopwords=True, remove_punctuation=True):\n",
    "    import string\n",
    "    \n",
    "    cleaned = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if remove_punctuation:\n",
    "            if token in string.punctuation or all(c in string.punctuation for c in token):\n",
    "                continue\n",
    "            token = ''.join(char for char in token if char not in string.punctuation)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            if token.lower() in french_stopwords:\n",
    "                continue\n",
    "        \n",
    "        if token.strip():\n",
    "            cleaned.append(token)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(\"netteoyage\\n\")\n",
    "\n",
    "tokens_to_clean = tokenize_words(text_normalized)\n",
    "\n",
    "print(f\"Tokens avant nettoyage: {len(tokens_to_clean)}\")\n",
    "print(f\"Premiers 20 tokens: {tokens_to_clean[:20]}\\n\")\n",
    "\n",
    "cleaned_tokens = clean_tokens(tokens_to_clean, remove_stopwords=True, remove_punctuation=True)\n",
    "\n",
    "print(f\"Tokens après nettoyage: {len(cleaned_tokens)}\")\n",
    "print(f\"Premiers 20 tokens nettoyés: {cleaned_tokens[:20]}\\n\")\n",
    "\n",
    "print(\"STATS\")\n",
    "print(f\"Tokens supprimés: {len(tokens_to_clean) - len(cleaned_tokens)}\")\n",
    "print(f\"Ratio de compression: {(len(tokens_to_clean) - len(cleaned_tokens)) / len(tokens_to_clean) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6197896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMING vs LEMMATISATION\n",
      "\n",
      "TEST SUR DES MOTS SPÉCIFIQUES\n",
      "\n",
      "Mot Original         | Stemming             | Lemmatisation (spaCy)    \n",
      "courant              | cour                 | courir                   \n",
      "courants             | courant              | courant                  \n",
      "couru                | couru                | courir                   \n",
      "courserait           | courser              | courser                  \n",
      "coureur              | coureur              | coureur                  \n",
      "coureurs             | coureur              | coureur                  \n",
      "marcher              | marcher              | marcher                  \n",
      "marcheurs            | marcheur             | marcheur                 \n",
      "marche               | march                | marche                   \n",
      "marchant             | march                | marchant                 \n",
      "marches              | marche               | marches                  \n",
      "livre                | livr                 | livre                    \n",
      "livres               | livre                | livre                    \n",
      "livreur              | livreur              | livreur                  \n",
      "livreurs             | livreur              | livreur                  \n",
      "livraison            | livraison            | livraison                \n",
      "intelligent          | intellig             | intelliger               \n",
      "intelligente         | intelligent          | intelligent              \n",
      "intelligents         | intelligent          | intelligent              \n",
      "intelligences        | intelligence         | intelligence             \n",
      "rapidement           | rapid                | rapidement               \n",
      "rapide               | rapid                | rapide                   \n",
      "rapidité             | rapidit              | rapidité                 \n",
      "rapidités            | rapidit              | rapidité                 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def simple_stemmer(word):\n",
    "    suffixes = [\n",
    "        'ement', 'ment', 'tion', 'sion',\n",
    "        'able', 'ible', 'ateur', 'atrice',\n",
    "        'eux', 'euse', 'ais', 'ait', 'aient',\n",
    "        'és', 'é', 'e', 's',\n",
    "        'et', 'elle', 'ette',\n",
    "        'ment',\n",
    "        'ant', 'ant', 'ent'\n",
    "    ]\n",
    "    \n",
    "    word_lower = word.lower()\n",
    "    \n",
    "    for suffix in sorted(suffixes, key=len, reverse=True):\n",
    "        if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 2:\n",
    "            return word_lower[:-len(suffix)]\n",
    "    \n",
    "    return word_lower\n",
    "\n",
    "def stem_words_list(words):\n",
    "    return [simple_stemmer(word) for word in words]\n",
    "\n",
    "\n",
    "def lemmatize_text_full(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "\n",
    "print(\"STEMMING vs LEMMATISATION\")\n",
    "\n",
    "test_words = [\n",
    "    'courant', 'courants', 'couru', 'courserait', 'coureur', 'coureurs',\n",
    "    'marcher', 'marcheurs', 'marche', 'marchant', 'marches',\n",
    "    'livre', 'livres', 'livreur', 'livreurs', 'livraison',\n",
    "    'intelligent', 'intelligente', 'intelligents', 'intelligences',\n",
    "    'rapidement', 'rapide', 'rapidité', 'rapidités'\n",
    "]\n",
    "\n",
    "print(\"\\nTEST SUR DES MOTS SPÉCIFIQUES\\n\")\n",
    "print(f\"{'Mot Original':<20} | {'Stemming':<20} | {'Lemmatisation (spaCy)':<25}\")\n",
    "\n",
    "for word in test_words:\n",
    "    stem = simple_stemmer(word)\n",
    "    lemma_result = lemmatize_text_full(word)\n",
    "    lemma = lemma_result[0] if lemma_result else word\n",
    "    print(f\"{word:<20} | {stem:<20} | {lemma:<25}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7a0a1414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens nettoyés: 156\n",
      "Tokens après stemming: 156\n",
      "Premiers 15 tokens nettoyés: ['noel', 'arrive', 'dit', 'noel', 'dit', 'nourriture', 'jeudi', 'huit', 'decembre', 'prochain', 'partir', 'midi', 'proposons', 'grand', 'buffet']\n",
      "Premiers 15 après stemming: ['noel', 'arriv', 'dit', 'noel', 'dit', 'nourritur', 'jeudi', 'huit', 'decembr', 'prochain', 'partir', 'midi', 'proposon', 'grand', 'buff']\n",
      "\n",
      "Tokens après lemmatisation: 148\n",
      "Premiers 30 après lemmatisation: ['noel', 'arriv', 'dire', 'noel', 'dire', 'nourriture', 'jeudi', 'huit', 'decembre', 'prochain', 'partir', 'midi', 'proposer', 'grand', 'buffet', 'collectif', 'noel', 'feter', 'ensemble', 'depart', 'vacance', 'hiver', 'premier', 'semestre', 'intense', 'voyage', 'disponible', 'servir', 'de', 'voir']\n",
      "\n",
      "\n",
      "Mots uniques dans les tokens nettoyés: 126\n",
      "Mots uniques après stemming: 125 (réduction de 0.8%)\n",
      "Mots uniques après lemmatisation: 112 (réduction de 11.1%)\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens = stem_words_list(cleaned_tokens)\n",
    "\n",
    "print(f\"Tokens nettoyés: {len(cleaned_tokens)}\")\n",
    "print(f\"Tokens après stemming: {len(stemmed_tokens)}\")\n",
    "print(f\"Premiers 15 tokens nettoyés: {cleaned_tokens[:15]}\")\n",
    "print(f\"Premiers 15 après stemming: {stemmed_tokens[:15]}\\n\")\n",
    "\n",
    "lemmatized_tokens = lemmatize_text_full(text_normalized)\n",
    "lemmatized_cleaned = [token.lemma_ for token in nlp(text_normalized) \n",
    "                      if token.text.lower() in [t.lower() for t in cleaned_tokens]]\n",
    "\n",
    "print(f\"Tokens après lemmatisation: {len(lemmatized_cleaned)}\")\n",
    "print(f\"Premiers 30 après lemmatisation: {lemmatized_cleaned[:30]}\\n\")\n",
    "\n",
    "unique_original = len(set(cleaned_tokens))\n",
    "unique_stemmed = len(set(stemmed_tokens))\n",
    "unique_lemmatized = len(set(lemmatized_cleaned))\n",
    "\n",
    "print(f\"\\nMots uniques dans les tokens nettoyés: {unique_original}\")\n",
    "print(f\"Mots uniques après stemming: {unique_stemmed} (réduction de {((unique_original - unique_stemmed) / unique_original * 100):.1f}%)\")\n",
    "print(f\"Mots uniques après lemmatisation: {unique_lemmatized} (réduction de {((unique_original - unique_lemmatized) / unique_original * 100):.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce0519",
   "metadata": {},
   "source": [
    "## Partie 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13d33bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire créé à partir des tokens nettoyés\n",
      "  Taille du vocabulaire: 126 mots uniques\n",
      "  Tokens nettoyés disponibles: 156\n",
      "\n",
      "Premiers 20 mots du vocabulaire: ['admiration', 'afin', 'alerte', 'alternance', 'arc', 'arrive', 'aura', 'automatiquement', 'autour', 'beaux', 'bien', 'bienvenus', 'buffet', 'cadeau', 'cadeaux', 'cakes', 'camarade', 'campus', 'ceux', 'chacun']\n",
      "\n",
      "Derniers 20 mots du vocabulaire: ['selon', 'semestre', 'sert', 'si', 'simple', 'sort', 'souhaitent', 'souhaitez', 'soyez', 'tartes', 'theme', 'tirage', 'tous', 'toutes', 'vacances', 'verrez', 'voyage', 'wagon', 'york', 'zero']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(cleaned_tokens))  \n",
    "vocab_sorted = sorted(vocab)\n",
    "\n",
    "print(f\"Vocabulaire créé à partir des tokens nettoyés\")\n",
    "print(f\"  Taille du vocabulaire: {len(vocab)} mots uniques\")\n",
    "print(f\"  Tokens nettoyés disponibles: {len(cleaned_tokens)}\\n\")\n",
    "\n",
    "print(f\"Premiers 20 mots du vocabulaire: {vocab_sorted[:20]}\\n\")\n",
    "print(f\"Derniers 20 mots du vocabulaire: {vocab_sorted[-20:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ddbc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " OneHotEncoder initialisé avec 126 mots\n",
      "Encodage du mot 'admiration':\n",
      "  Vecteur: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  Index du 1: 0\n",
      "\n",
      "Encodage des mots ['admiration', 'afin', 'alerte', 'alternance', 'arc']:\n",
      "  admiration      →   0 (1 à la position 0)\n",
      "  afin            →   1 (1 à la position 1)\n",
      "  alerte          →   2 (1 à la position 2)\n",
      "  alternance      →   3 (1 à la position 3)\n",
      "  arc             →   4 (1 à la position 4)\n",
      "\n",
      "Décodage des vecteurs:\n",
      "  Vecteur → admiration\n",
      "  Vecteur → afin\n",
      "  Vecteur → alerte\n",
      "  Vecteur → alternance\n",
      "  Vecteur → arc\n",
      "Taille du vocabulaire: 126 mots\n",
      "Dimension de chaque vecteur one-hot: 126\n"
     ]
    }
   ],
   "source": [
    "class OneHotEncoder:\n",
    "    def __init__(self, vocabulary):\n",
    "\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        print(f\" OneHotEncoder initialisé avec {self.vocab_size} mots\")\n",
    "    \n",
    "    def encode_word(self, word):\n",
    "        if word not in self.vocabulary:\n",
    "            raise ValueError(f\"Mot '{word}' non trouvé dans le vocabulaire\")\n",
    "\n",
    "        one_hot_vector = [0] * self.vocab_size\n",
    "        one_hot_vector[self.vocabulary[word]] = 1\n",
    "        \n",
    "        return one_hot_vector\n",
    "    \n",
    "    def encode_sentence(self, words):\n",
    "        encoded = []\n",
    "        for word in words:\n",
    "            if word in self.vocabulary:\n",
    "                encoded.append(self.encode_word(word))\n",
    "        return encoded\n",
    "    \n",
    "    def decode_vector(self, vector):\n",
    "\n",
    "        if vector.count(1) != 1:\n",
    "            return None\n",
    "        \n",
    "        index = vector.index(1)\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            if idx == index:\n",
    "                return word\n",
    "        return None\n",
    "    \n",
    "    def get_vocabulary_info(self):\n",
    "        return {\n",
    "            'size': self.vocab_size,\n",
    "            'vocabulary': self.vocabulary\n",
    "        }\n",
    "\n",
    "if vocab:\n",
    "    encoder = OneHotEncoder(vocab)\n",
    "    \n",
    "    test_word = vocab_sorted[0]\n",
    "    print(f\"Encodage du mot '{test_word}':\")\n",
    "    encoded = encoder.encode_word(test_word)\n",
    "    print(f\"  Vecteur: {encoded}\")\n",
    "    print(f\"  Index du 1: {encoded.index(1)}\\n\")\n",
    "    \n",
    "    test_words = vocab_sorted[:5]\n",
    "    print(f\"Encodage des mots {test_words}:\")\n",
    "    for word in test_words:\n",
    "        vec = encoder.encode_word(word)\n",
    "        print(f\"  {word:<15} → {vec.index(1):3d} (1 à la position {vec.index(1)})\")\n",
    "    \n",
    "    print(f\"\\nDécodage des vecteurs:\")\n",
    "    for word in test_words:\n",
    "        vec = encoder.encode_word(word)\n",
    "        decoded = encoder.decode_vector(vec)\n",
    "        print(f\"  Vecteur → {decoded}\")\n",
    "    \n",
    "    print(f\"Taille du vocabulaire: {encoder.vocab_size} mots\")\n",
    "    print(f\"Dimension de chaque vecteur one-hot: {encoder.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c94329ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BagOfWords initialisé avec 126 mots\n",
      "Document: 156 mots\n",
      "Vecteur BoW créé avec 126 dimensions\n",
      "\n",
      "Nombre de mots uniques avec fréquence > 0: 126\n",
      "\n",
      "Top 15 mots les plus fréquents:\n",
      "────────────────────────────────────────\n",
      "noel            |   7 |  4.49% ██\n",
      "du              |   3 |  1.92% \n",
      "ensemble        |   3 |  1.92% \n",
      "plus            |   3 |  1.92% \n",
      "bienvenus       |   2 |  1.28% \n",
      "collectif       |   2 |  1.28% \n",
      "decembre        |   2 |  1.28% \n",
      "dit             |   2 |  1.28% \n",
      "etes            |   2 |  1.28% \n",
      "grand           |   2 |  1.28% \n",
      "huit            |   2 |  1.28% \n",
      "jeudi           |   2 |  1.28% \n",
      "moment          |   2 |  1.28% \n",
      "offrir          |   2 |  1.28% \n",
      "prochain        |   2 |  1.28% \n",
      "Vecteur BoW - Première moitié: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Nombre total de mots: 156\n",
      "Moyenne d'occurrences par mot: 1.24\n",
      "Sparsité: 0.00% de zéros\n"
     ]
    }
   ],
   "source": [
    "class BagOfWords:    \n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        print(f\"BagOfWords initialisé avec {self.vocab_size} mots\")\n",
    "    \n",
    "    def encode_document(self, words):\n",
    "        bow_vector = [0] * self.vocab_size\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.vocabulary:\n",
    "                idx = self.vocabulary[word]\n",
    "                bow_vector[idx] += 1\n",
    "        \n",
    "        return bow_vector\n",
    "    \n",
    "    def encode_documents(self, documents):\n",
    "        encoded_docs = []\n",
    "        for doc in documents:\n",
    "            encoded_docs.append(self.encode_document(doc))\n",
    "        return encoded_docs\n",
    "    \n",
    "    def get_word_frequencies(self, vector):\n",
    "        frequencies = {}\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            if vector[idx] > 0:\n",
    "                frequencies[word] = vector[idx]\n",
    "        return frequencies\n",
    "    \n",
    "    def get_top_words(self, vector, top_n=10):\n",
    "        frequencies = self.get_word_frequencies(vector)\n",
    "        sorted_freq = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "        return sorted_freq[:top_n]\n",
    "    \n",
    "    def get_vocabulary_info(self):\n",
    "        return {\n",
    "            'size': self.vocab_size,\n",
    "            'vocabulary': self.vocabulary\n",
    "        }\n",
    "\n",
    "\n",
    "if vocab:\n",
    "    bow = BagOfWords(vocab)\n",
    "    bow_vector = bow.encode_document(cleaned_tokens)\n",
    "    \n",
    "    print(f\"Document: {len(cleaned_tokens)} mots\")\n",
    "    print(f\"Vecteur BoW créé avec {bow.vocab_size} dimensions\\n\")\n",
    "    \n",
    "    frequencies = bow.get_word_frequencies(bow_vector)\n",
    "    print(f\"Nombre de mots uniques avec fréquence > 0: {len(frequencies)}\\n\")\n",
    "    \n",
    "    print(\"Top 15 mots les plus fréquents:\")\n",
    "    print(\"─\" * 40)\n",
    "    top_words = bow.get_top_words(bow_vector, 15)\n",
    "    for word, freq in top_words:\n",
    "        pourcentage = (freq / sum(bow_vector)) * 100\n",
    "        barre = \"█\" * int(pourcentage / 2)\n",
    "        print(f\"{word:<15} | {freq:3d} | {pourcentage:5.2f}% {barre}\")\n",
    "    \n",
    "    print(f\"Vecteur BoW - Première moitié: {bow_vector[:10]}\")\n",
    "    print(f\"Nombre total de mots: {sum(bow_vector)}\")\n",
    "    print(f\"Moyenne d'occurrences par mot: {sum(bow_vector) / len(frequencies):.2f}\")\n",
    "    print(f\"Sparsité: {(len([x for x in bow_vector if x == 0]) / len(bow_vector) * 100):.2f}% de zéros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd39eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CBOW initialisé\n",
      "  Vocabulaire: 126 mots\n",
      "  Dimension des embeddings: 10\n",
      "  Taille du contexte: ±2 mots\n",
      "\n",
      " Skip-Gram initialisé\n",
      "  Vocabulaire: 126 mots\n",
      "  Dimension des embeddings: 10\n",
      "  Taille du contexte: ±2 mots\n",
      "\n",
      " Distributed Bag of Words initialisé\n",
      "  Vocabulaire: 126 mots\n",
      "  Dimension des embeddings: 20\n",
      "\n",
      " Distributed Memory initialisé\n",
      "  Vocabulaire: 126 mots\n",
      "  Dimension des embeddings: 20\n",
      "  Taille du contexte: ±2 mots\n",
      "\n",
      "Prédit un mot à partir du contexte environnant\n",
      "\n",
      "Fenêtres de contexte extraites: 50\n",
      "\n",
      "Exemple:\n",
      "  Contexte: ['arrive', 'dit']\n",
      "  Cible réelle: noel\n",
      "  Top 5 prédictions:\n",
      "    esprit          score: 2.5103\n",
      "    intense         score: 2.3884\n",
      "    mettez          score: 2.2822\n",
      "    egalement       score: 1.9711\n",
      "    repas           score: 1.9422\n",
      "Prédit le contexte à partir du mot cible\n",
      "\n",
      "Paires (target, contexte) créées: 194\n",
      "\n",
      "Exemple:\n",
      "  Mot cible: noel\n",
      "  Contexte réel: arrive\n",
      "  Top 5 contextes prédits:\n",
      "    sapin           score: 3.7805\n",
      "    partir          score: 3.2899\n",
      "    dit             score: 3.1178\n",
      "    possibilites    score: 3.0251\n",
      "    simple          score: 2.9209\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class ContinuousBagOfWords:\n",
    "    def __init__(self, vocabulary, window_size=2, embedding_dim=10):\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        print(f\" CBOW initialisé\")\n",
    "        print(f\"  Vocabulaire: {self.vocab_size} mots\")\n",
    "        print(f\"  Dimension des embeddings: {embedding_dim}\")\n",
    "        print(f\"  Taille du contexte: ±{window_size} mots\\n\")\n",
    "    \n",
    "    def get_context_windows(self, words):\n",
    "        windows = []\n",
    "        for i in range(len(words)):\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(words), i + self.window_size + 1)\n",
    "            \n",
    "            context = []\n",
    "            for j in range(start, end):\n",
    "                if j != i and words[j] in self.vocabulary:\n",
    "                    context.append(words[j])\n",
    "            \n",
    "            if context and words[i] in self.vocabulary:\n",
    "                windows.append((context, words[i]))\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            idx = self.vocabulary[word]\n",
    "            return self.embeddings[idx]\n",
    "        return None\n",
    "    \n",
    "    def get_context_vector(self, context_words):\n",
    "        vectors = [self.get_embedding(w) for w in context_words if w in self.vocabulary]\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        return np.zeros(self.embedding_dim)\n",
    "    \n",
    "    def predict_word_from_context(self, context_words, top_n=5):\n",
    "        context_vec = self.get_context_vector(context_words)\n",
    "        \n",
    "        similarities = []\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            sim = np.dot(context_vec, self.embeddings[idx])\n",
    "            similarities.append((word, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, vocabulary, window_size=2, embedding_dim=10):\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.input_embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        self.output_embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        print(f\" Skip-Gram initialisé\")\n",
    "        print(f\"  Vocabulaire: {self.vocab_size} mots\")\n",
    "        print(f\"  Dimension des embeddings: {embedding_dim}\")\n",
    "        print(f\"  Taille du contexte: ±{window_size} mots\\n\")\n",
    "    \n",
    "    def get_training_pairs(self, words):\n",
    "        pairs = []\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in self.vocabulary:\n",
    "                continue\n",
    "            \n",
    "            target = words[i]\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(words), i + self.window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if j != i and words[j] in self.vocabulary:\n",
    "                    pairs.append((target, words[j]))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def get_embedding(self, word):\n",
    "        if word in self.vocabulary:\n",
    "            idx = self.vocabulary[word]\n",
    "            return self.input_embeddings[idx]\n",
    "        return None\n",
    "    \n",
    "    def predict_context(self, target_word, top_n=5):\n",
    "        if target_word not in self.vocabulary:\n",
    "            return []\n",
    "        \n",
    "        target_idx = self.vocabulary[target_word]\n",
    "        target_vec = self.input_embeddings[target_idx]\n",
    "        \n",
    "        similarities = []\n",
    "        for word, idx in self.vocabulary.items():\n",
    "            sim = np.dot(target_vec, self.output_embeddings[idx])\n",
    "            similarities.append((word, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]\n",
    "\n",
    "\n",
    "class DistributedBagOfWords:\n",
    "    def __init__(self, vocabulary, embedding_dim=20):\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.word_embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        print(f\" Distributed Bag of Words initialisé\")\n",
    "        print(f\"  Vocabulaire: {self.vocab_size} mots\")\n",
    "        print(f\"  Dimension des embeddings: {embedding_dim}\\n\")\n",
    "    \n",
    "    def get_document_vector(self, words, doc_id=None):\n",
    "        word_vecs = [self.word_embeddings[self.vocabulary[w]] \n",
    "                     for w in words if w in self.vocabulary]\n",
    "        \n",
    "        if word_vecs:\n",
    "            doc_vec = np.mean(word_vecs, axis=0)\n",
    "        else:\n",
    "            doc_vec = np.zeros(self.embedding_dim)\n",
    "        \n",
    "        return doc_vec\n",
    "    \n",
    "    def get_document_similarity(self, doc1_words, doc2_words):\n",
    "        vec1 = self.get_document_vector(doc1_words)\n",
    "        vec2 = self.get_document_vector(doc2_words)\n",
    "        \n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm1 > 0 and norm2 > 0:\n",
    "            return np.dot(vec1, vec2) / (norm1 * norm2)\n",
    "        return 0.0\n",
    "\n",
    "class DistributedMemory:\n",
    "    def __init__(self, vocabulary, window_size=2, embedding_dim=20):\n",
    "        self.vocabulary = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "        self.vocab_size = len(self.vocabulary)\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        self.word_embeddings = np.random.randn(self.vocab_size, embedding_dim)\n",
    "        \n",
    "        print(f\" Distributed Memory initialisé\")\n",
    "        print(f\"  Vocabulaire: {self.vocab_size} mots\")\n",
    "        print(f\"  Dimension des embeddings: {embedding_dim}\")\n",
    "        print(f\"  Taille du contexte: ±{window_size} mots\\n\")\n",
    "    \n",
    "    def get_context_windows_with_doc(self, words, doc_id=0):\n",
    "        windows = []\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in self.vocabulary:\n",
    "                continue\n",
    "            \n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(len(words), i + self.window_size + 1)\n",
    "            \n",
    "            context = []\n",
    "            for j in range(start, end):\n",
    "                if j != i and words[j] in self.vocabulary:\n",
    "                    context.append(words[j])\n",
    "            \n",
    "            if context:\n",
    "                context_with_doc = context + [f\"DOC_{doc_id}\"]\n",
    "                windows.append((context_with_doc, words[i]))\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def get_document_representation(self, words, doc_id=0):\n",
    "        word_vecs = [self.word_embeddings[self.vocabulary[w]] \n",
    "                     for w in words if w in self.vocabulary]\n",
    "        \n",
    "        if word_vecs:\n",
    "            return np.mean(word_vecs, axis=0)\n",
    "        return np.zeros(self.embedding_dim)\n",
    "\n",
    "if vocab and cleaned_tokens:\n",
    "    cbow = ContinuousBagOfWords(vocab, window_size=2, embedding_dim=10)\n",
    "    skipgram = SkipGram(vocab, window_size=2, embedding_dim=10)\n",
    "    dbow = DistributedBagOfWords(vocab, embedding_dim=20)\n",
    "    dm = DistributedMemory(vocab, window_size=2, embedding_dim=20)\n",
    "    \n",
    "    sample_words = cleaned_tokens[:50]\n",
    "    \n",
    "    print(\"Prédit un mot à partir du contexte environnant\\n\")\n",
    "    \n",
    "    cbow_windows = cbow.get_context_windows(sample_words)\n",
    "    print(f\"Fenêtres de contexte extraites: {len(cbow_windows)}\\n\")\n",
    "\n",
    "    if cbow_windows:\n",
    "        context, target = cbow_windows[0]\n",
    "        print(f\"Exemple:\")\n",
    "        print(f\"  Contexte: {context}\")\n",
    "        print(f\"  Cible réelle: {target}\")\n",
    "        predictions = cbow.predict_word_from_context(context, top_n=5)\n",
    "        print(f\"  Top 5 prédictions:\")\n",
    "        for word, score in predictions:\n",
    "            print(f\"    {word:<15} score: {score:.4f}\")\n",
    "\n",
    "    print(\"Prédit le contexte à partir du mot cible\\n\")\n",
    "    \n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b335fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paires (target, contexte) créées: 194\n",
      "\n",
      "Exemple:\n",
      "  Mot cible: noel\n",
      "  Contexte réel: arrive\n",
      "  Top 5 contextes prédits:\n",
      "    sapin           score: 3.7805\n",
      "    partir          score: 3.2899\n",
      "    dit             score: 3.1178\n",
      "    possibilites    score: 3.0251\n",
      "    simple          score: 2.9209\n"
     ]
    }
   ],
   "source": [
    "if vocab and cleaned_tokens:\n",
    "    skipgram_pairs = skipgram.get_training_pairs(sample_words)\n",
    "    print(f\"Paires (target, contexte) créées: {len(skipgram_pairs)}\\n\")\n",
    "\n",
    "    if skipgram_pairs:\n",
    "        target, context = skipgram_pairs[0]\n",
    "        print(f\"Exemple:\")\n",
    "        print(f\"  Mot cible: {target}\")\n",
    "        print(f\"  Contexte réel: {context}\")\n",
    "        predictions = skipgram.predict_context(target, top_n=5)\n",
    "        print(f\"  Top 5 contextes prédits:\")\n",
    "        for word, score in predictions:\n",
    "            print(f\"    {word:<15} score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ac3cc32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Représente un document par un vecteur continu\n",
      "\n",
      "Vecteur document 1: [ 0.25120644  0.3022648   0.09942717 -0.09420201 -0.07429434]... (taille: (20,))\n",
      "Vecteur document 2: [ 0.2168211   0.1556571  -0.06612796 -0.00911701 -0.11699125]... (taille: (20,))\n",
      "Vecteur document 3: [ 0.3949813  -0.14876373  0.06921144 -0.03011026 -0.1838882 ]... (taille: (20,))\n",
      "\n",
      "Similarités cosinus entre documents:\n",
      "  Doc1 vs Doc2: 0.5054\n",
      "  Doc1 vs Doc3: 0.3999\n",
      "  Doc2 vs Doc3: 0.3598\n",
      "Combine contexte ET ID du document pour prédire\n",
      "\n",
      "Fenêtres (contexte + doc_id, target) créées: 30\n",
      "\n",
      "Exemple:\n",
      "  Contexte + DocID: ['arrive', 'dit', 'DOC_1']\n",
      "  Cible: noel\n",
      "  Représentation du doc: [ 0.25120644  0.3022648   0.09942717 -0.09420201 -0.07429434]... (taille: (20,))\n"
     ]
    }
   ],
   "source": [
    "if vocab and cleaned_tokens:\n",
    "    \n",
    "    print(\"Représente un document par un vecteur continu\\n\")\n",
    "  \n",
    "    doc1 = cleaned_tokens[:30]\n",
    "    doc2 = cleaned_tokens[30:60]\n",
    "    doc3 = cleaned_tokens[60:90]\n",
    "    \n",
    "    vec1 = dbow.get_document_vector(doc1)\n",
    "    vec2 = dbow.get_document_vector(doc2)\n",
    "    vec3 = dbow.get_document_vector(doc3)\n",
    "    \n",
    "    print(f\"Vecteur document 1: {vec1[:5]}... (taille: {vec1.shape})\")\n",
    "    print(f\"Vecteur document 2: {vec2[:5]}... (taille: {vec2.shape})\")\n",
    "    print(f\"Vecteur document 3: {vec3[:5]}... (taille: {vec3.shape})\\n\")\n",
    "    sim_1_2 = dbow.get_document_similarity(doc1, doc2)\n",
    "    sim_1_3 = dbow.get_document_similarity(doc1, doc3)\n",
    "    sim_2_3 = dbow.get_document_similarity(doc2, doc3)\n",
    "    \n",
    "    print(\"Similarités cosinus entre documents:\")\n",
    "    print(f\"  Doc1 vs Doc2: {sim_1_2:.4f}\")\n",
    "    print(f\"  Doc1 vs Doc3: {sim_1_3:.4f}\")\n",
    "    print(f\"  Doc2 vs Doc3: {sim_2_3:.4f}\")\n",
    "\n",
    "    print(\"Combine contexte ET ID du document pour prédire\\n\")\n",
    "    \n",
    "    dm_windows = dm.get_context_windows_with_doc(sample_words[:30], doc_id=1)\n",
    "    print(f\"Fenêtres (contexte + doc_id, target) créées: {len(dm_windows)}\\n\")\n",
    "    \n",
    "    if dm_windows:\n",
    "        context, target = dm_windows[0]\n",
    "        print(f\"Exemple:\")\n",
    "        print(f\"  Contexte + DocID: {context}\")\n",
    "        print(f\"  Cible: {target}\")\n",
    "    \n",
    "    doc_repr = dm.get_document_representation(sample_words[:30], doc_id=1)\n",
    "    print(f\"  Représentation du doc: {doc_repr[:5]}... (taille: {doc_repr.shape})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

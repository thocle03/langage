{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01ee15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bibliothèques installées avec succès\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['nltk', 'spacy', 'unidecode', 'numpy', 'pandas']\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "print(\"Bibliothèques installées avec succès\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00a803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement du modèle spaCy français...\n",
      "Imports et modèles chargés\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "except OSError:\n",
    "    print(\"Téléchargement du modèle spaCy français...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'spacy', 'download', 'fr_core_news_sm'])\n",
    "    nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "print(\"Imports et modèles chargés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c67333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte chargé: 1616 caractères\n",
      "\n",
      "--- Premier 200 caractères ---\n",
      "OH OH OH, Noel arrive, et qui dit Noel dit ... nourriture !\n",
      "Jeudi 18 décembre prochain, à partir de midi, nous vous proposons un grand buffet collectif de noel pour fêter ensemble vos départs en vacan...\n"
     ]
    }
   ],
   "source": [
    "def load_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Erreur: Le fichier {file_path} n'existe pas\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "text = load_text_file('entree.txt')\n",
    "print(f\"Texte chargé: {len(text)} caractères\")\n",
    "print(\"\\n--- Premier 200 caractères ---\")\n",
    "print(text[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bc92c74",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\thoma/nltk_data'\n    - 'c:\\\\Users\\\\thoma\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\thoma\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\thoma\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\thoma\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\thoma\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[1;32m----> 5\u001b[0m words \u001b[38;5;241m=\u001b[39m tokenize_words1(text)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNombre de mots: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(words)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPremiers 20 mots: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords[:\u001b[38;5;241m20\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m, in \u001b[0;36mtokenize_words1\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_words1\u001b[39m(text):\n\u001b[1;32m----> 2\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrench\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mc:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\thoma/nltk_data'\n    - 'c:\\\\Users\\\\thoma\\\\anaconda3\\\\nltk_data'\n    - 'c:\\\\Users\\\\thoma\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\thoma\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\thoma\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\thoma\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "def tokenize_words1(text):\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    return tokens\n",
    "\n",
    "words = tokenize_words1(text)\n",
    "print(f\"Nombre de mots: {len(words)}\")\n",
    "print(f\"Premiers 20 mots: {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1213db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ressource NLTK manquante\n",
      "Nombre de mots: 316\n",
      "Premiers 20 mots: ['OH', 'OH', 'OH', ',', 'Noel', 'arrive', ',', 'et', 'qui', 'dit', 'Noel', 'dit', '.', '.', '.', 'nourriture', '!', 'Jeudi', '18', 'décembre']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_words(text):\n",
    "    print(f\"Ressource NLTK manquante\")\n",
    "    import re\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[.,!?;:\\-]', text)\n",
    "    return tokens\n",
    "\n",
    "words = tokenize_words(text)\n",
    "print(f\"Nombre de mots: {len(words)}\")\n",
    "print(f\"Premiers 20 mots: {words[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85b92f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrases: 13\n",
      "\n",
      "--- Premières 3 phrases ---\n",
      "1. OH OH OH, Noel arrive, et qui dit Noel dit \n",
      "2.  nourriture \n",
      "3. Jeudi 18 décembre prochain, à partir de midi, nous vous proposons un grand buffet collectif de noel pour fêter ensemble vos départs en vacances d'hiver après un premier semestre intense \n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(text):\n",
    "    sentences = re.split(r'\\.\\.\\.|[.!?]\\s*', text)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "sentences = tokenize_sentences(text)\n",
    "print(f\"Nombre de phrases: {len(sentences)}\")\n",
    "print(\"\\n--- Premières 3 phrases ---\")\n",
    "for i, sent in enumerate(sentences[:3], 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0515f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de bigrammes/trigrammes (séparés par -): 6\n",
      "les 10 premiers bi/tri grams: [('arc', 'en', 'ciel'), ('New', 'York'), ('wagon', 'restaurant'), ('chou', 'fleur'), ('arc', 'e'), ('n', 'ciel')]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_bigrams(text):\n",
    "    ngrams = []\n",
    "    trigrammes = re.findall(r'(\\w+)-(\\w+)-(\\w+)', text)\n",
    "    for tri in trigrammes:\n",
    "        ngrams.append(tri)  \n",
    "    \n",
    "    bigrammes = re.findall(r'(\\w+)-(\\w+)(?!-)', text)\n",
    "    for bi in bigrammes:\n",
    "        ngrams.append(bi) \n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "bigrams = tokenize_bigrams(text)\n",
    "print(f\"Nombre de bigrammes/trigrammes (séparés par -): {len(bigrams)}\")\n",
    "\n",
    "print (f\"les 10 premiers bi/tri grammes: {bigrams[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c93f9c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST \n",
      "\n",
      "Original:  Noël est le 25 décembre! C'est une fêtE avec Événements.\n",
      "Normalisé: noel est le deux cinq  decembre! c'est une fete avec evenements.\n",
      "Nombre de caractères: 1616 → 1634\n",
      "\n",
      "Premier 200 caractères normalisés:\n",
      "oh oh oh, noel arrive, et qui dit noel dit ... nourriture !\n",
      "jeudi un huit  decembre prochain, a partir de midi, nous vous proposons un grand buffet collectif de noel pour feter ensemble vos departs en...\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    import unicodedata\n",
    "    text = text.lower()\n",
    "    \n",
    "    text_nfd = unicodedata.normalize('NFD', text)\n",
    "    text_without_accents = ''.join(char for char in text_nfd \n",
    "                                    if unicodedata.category(char) != 'Mn')\n",
    "    nombres_texte = {\n",
    "        '0': 'zero', '1': 'un', '2': 'deux', '3': 'trois', '4': 'quatre',\n",
    "        '5': 'cinq', '6': 'six', '7': 'sept', '8': 'huit', '9': 'neuf'\n",
    "    }\n",
    "    \n",
    "    text_normalized = ''\n",
    "    for char in text_without_accents:\n",
    "        if char.isdigit():\n",
    "            text_normalized += nombres_texte[char] + ' '\n",
    "        else:\n",
    "            text_normalized += char\n",
    "    \n",
    "    return text_normalized.strip()\n",
    "\n",
    "print(\"TEST \\n\")\n",
    "\n",
    "sample_text = \"Noël est le 25 décembre! C'est une fêtE avec Événements.\"\n",
    "normalized = normalize_text(sample_text)\n",
    "\n",
    "print(f\"Original:  {sample_text}\")\n",
    "print(f\"Normalisé: {normalized}\")\n",
    "\n",
    "text_normalized = normalize_text(text)\n",
    "print(f\"Nombre de caractères: {len(text)} → {len(text_normalized)}\")\n",
    "print(f\"\\nPremier 200 caractères normalisés:\")\n",
    "print(text_normalized[:200] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98708590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netteoyage\n",
      "\n",
      "Ressource NLTK manquante\n",
      "Tokens avant nettoyage: 319\n",
      "Premiers 20 tokens: ['oh', 'oh', 'oh', ',', 'noel', 'arrive', ',', 'et', 'qui', 'dit', 'noel', 'dit', '.', '.', '.', 'nourriture', '!', 'jeudi', 'un', 'huit']\n",
      "\n",
      "Tokens après nettoyage: 156\n",
      "Premiers 20 tokens nettoyés: ['noel', 'arrive', 'dit', 'noel', 'dit', 'nourriture', 'jeudi', 'huit', 'decembre', 'prochain', 'partir', 'midi', 'proposons', 'grand', 'buffet', 'collectif', 'noel', 'feter', 'ensemble', 'departs']\n",
      "\n",
      "STATS\n",
      "Tokens supprimés: 163\n",
      "Ratio de compression: 51.1%\n"
     ]
    }
   ],
   "source": [
    "french_stopwords = {\n",
    "    'le', 'la', 'les', 'de', 'des', 'un', 'une', 'et', 'ou', 'mais', 'donc',\n",
    "    'car', 'par', 'pour', 'avec', 'sans', 'sur', 'sous', 'dans', 'entre',\n",
    "    'avant', 'apres', 'pendant', 'lors', 'qui', 'que', 'quoi', 'quel', 'quelle',\n",
    "    'dont', 'duquel', 'auquel', 'a', 'au', 'aux', 'ce', 'cet', 'cette', 'ces',\n",
    "    'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles', 'on', 'mon', 'ton',\n",
    "    'son', 'notre', 'votre', 'leur', 'mes', 'tes', 'ses', 'nos', 'vos', 'leurs',\n",
    "    'moi', 'toi', 'lui', 'me', 'te', 'se', 'nous', 'vous', 'leur', 'est', 'etre',\n",
    "    'avoir', 'avoir', 'faire', 'aller', 'venir', 'pouvoir', 'devoir', 'vouloir',\n",
    "    'c', 'est', 'd', 'l', 'j', 's', 't', 'n', 'm', 'y', 'en', 'a', 'as', 'oh'\n",
    "}\n",
    "\n",
    "def clean_tokens(tokens, remove_stopwords=True, remove_punctuation=True):\n",
    "    import string\n",
    "    \n",
    "    cleaned = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if remove_punctuation:\n",
    "            if token in string.punctuation or all(c in string.punctuation for c in token):\n",
    "                continue\n",
    "            token = ''.join(char for char in token if char not in string.punctuation)\n",
    "        \n",
    "        if remove_stopwords:\n",
    "            if token.lower() in french_stopwords:\n",
    "                continue\n",
    "        \n",
    "        if token.strip():\n",
    "            cleaned.append(token)\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "print(\"netteoyage\\n\")\n",
    "\n",
    "tokens_to_clean = tokenize_words(text_normalized)\n",
    "\n",
    "print(f\"Tokens avant nettoyage: {len(tokens_to_clean)}\")\n",
    "print(f\"Premiers 20 tokens: {tokens_to_clean[:20]}\\n\")\n",
    "\n",
    "cleaned_tokens = clean_tokens(tokens_to_clean, remove_stopwords=True, remove_punctuation=True)\n",
    "\n",
    "print(f\"Tokens après nettoyage: {len(cleaned_tokens)}\")\n",
    "print(f\"Premiers 20 tokens nettoyés: {cleaned_tokens[:20]}\\n\")\n",
    "\n",
    "print(\"STATS\")\n",
    "print(f\"Tokens supprimés: {len(tokens_to_clean) - len(cleaned_tokens)}\")\n",
    "print(f\"Ratio de compression: {(len(tokens_to_clean) - len(cleaned_tokens)) / len(tokens_to_clean) * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6197896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEMMING vs LEMMATISATION\n",
      "\n",
      "TEST SUR DES MOTS SPÉCIFIQUES\n",
      "\n",
      "Mot Original         | Stemming             | Lemmatisation (spaCy)    \n",
      "courant              | cour                 | courir                   \n",
      "courants             | courant              | courant                  \n",
      "couru                | couru                | courir                   \n",
      "courserait           | courser              | courser                  \n",
      "coureur              | coureur              | coureur                  \n",
      "coureurs             | coureur              | coureur                  \n",
      "marcher              | marcher              | marcher                  \n",
      "marcheurs            | marcheur             | marcheur                 \n",
      "marche               | march                | marche                   \n",
      "marchant             | march                | marchant                 \n",
      "marches              | marche               | marches                  \n",
      "livre                | livr                 | livre                    \n",
      "livres               | livre                | livre                    \n",
      "livreur              | livreur              | livreur                  \n",
      "livreurs             | livreur              | livreur                  \n",
      "livraison            | livraison            | livraison                \n",
      "intelligent          | intellig             | intelliger               \n",
      "intelligente         | intelligent          | intelligent              \n",
      "intelligents         | intelligent          | intelligent              \n",
      "intelligences        | intelligence         | intelligence             \n",
      "rapidement           | rapid                | rapidement               \n",
      "rapide               | rapid                | rapide                   \n",
      "rapidité             | rapidit              | rapidité                 \n",
      "rapidités            | rapidit              | rapidité                 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def simple_stemmer(word):\n",
    "    suffixes = [\n",
    "        'ement', 'ment', 'tion', 'sion',\n",
    "        'able', 'ible', 'ateur', 'atrice',\n",
    "        'eux', 'euse', 'ais', 'ait', 'aient',\n",
    "        'és', 'é', 'e', 's',\n",
    "        'et', 'elle', 'ette',\n",
    "        'ment',\n",
    "        'ant', 'ant', 'ent'\n",
    "    ]\n",
    "    \n",
    "    word_lower = word.lower()\n",
    "    \n",
    "    for suffix in sorted(suffixes, key=len, reverse=True):\n",
    "        if word_lower.endswith(suffix) and len(word_lower) > len(suffix) + 2:\n",
    "            return word_lower[:-len(suffix)]\n",
    "    \n",
    "    return word_lower\n",
    "\n",
    "def stem_words_list(words):\n",
    "    return [simple_stemmer(word) for word in words]\n",
    "\n",
    "\n",
    "def lemmatize_text_full(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "\n",
    "print(\"STEMMING vs LEMMATISATION\")\n",
    "\n",
    "test_words = [\n",
    "    'courant', 'courants', 'couru', 'courserait', 'coureur', 'coureurs',\n",
    "    'marcher', 'marcheurs', 'marche', 'marchant', 'marches',\n",
    "    'livre', 'livres', 'livreur', 'livreurs', 'livraison',\n",
    "    'intelligent', 'intelligente', 'intelligents', 'intelligences',\n",
    "    'rapidement', 'rapide', 'rapidité', 'rapidités'\n",
    "]\n",
    "\n",
    "print(\"\\nTEST SUR DES MOTS SPÉCIFIQUES\\n\")\n",
    "print(f\"{'Mot Original':<20} | {'Stemming':<20} | {'Lemmatisation (spaCy)':<25}\")\n",
    "\n",
    "for word in test_words:\n",
    "    stem = simple_stemmer(word)\n",
    "    lemma_result = lemmatize_text_full(word)\n",
    "    lemma = lemma_result[0] if lemma_result else word\n",
    "    print(f\"{word:<20} | {stem:<20} | {lemma:<25}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a0a1414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens nettoyés: 156\n",
      "Tokens après stemming: 156\n",
      "Premiers 15 tokens nettoyés: ['noel', 'arrive', 'dit', 'noel', 'dit', 'nourriture', 'jeudi', 'huit', 'decembre', 'prochain', 'partir', 'midi', 'proposons', 'grand', 'buffet']\n",
      "Premiers 15 après stemming: ['noel', 'arriv', 'dit', 'noel', 'dit', 'nourritur', 'jeudi', 'huit', 'decembr', 'prochain', 'partir', 'midi', 'proposon', 'grand', 'buff']\n",
      "\n",
      "Tokens après lemmatisation: 148\n",
      "Premiers 15 après lemmatisation: ['noel', 'arriv', 'dire', 'noel', 'dire', 'nourriture', 'jeudi', 'huit', 'decembre', 'prochain', 'partir', 'midi', 'proposer', 'grand', 'buffet']\n",
      "\n",
      "\n",
      "Mots uniques dans les tokens nettoyés: 126\n",
      "Mots uniques après stemming: 125 (réduction de 0.8%)\n",
      "Mots uniques après lemmatisation: 112 (réduction de 11.1%)\n"
     ]
    }
   ],
   "source": [
    "stemmed_tokens = stem_words_list(cleaned_tokens)\n",
    "\n",
    "print(f\"Tokens nettoyés: {len(cleaned_tokens)}\")\n",
    "print(f\"Tokens après stemming: {len(stemmed_tokens)}\")\n",
    "print(f\"Premiers 15 tokens nettoyés: {cleaned_tokens[:15]}\")\n",
    "print(f\"Premiers 15 après stemming: {stemmed_tokens[:15]}\\n\")\n",
    "\n",
    "lemmatized_tokens = lemmatize_text_full(text_normalized)\n",
    "lemmatized_cleaned = [token.lemma_ for token in nlp(text_normalized) \n",
    "                      if token.text.lower() in [t.lower() for t in cleaned_tokens]]\n",
    "\n",
    "print(f\"Tokens après lemmatisation: {len(lemmatized_cleaned)}\")\n",
    "print(f\"Premiers 15 après lemmatisation: {lemmatized_cleaned[:15]}\\n\")\n",
    "\n",
    "unique_original = len(set(cleaned_tokens))\n",
    "unique_stemmed = len(set(stemmed_tokens))\n",
    "unique_lemmatized = len(set(lemmatized_cleaned))\n",
    "\n",
    "print(f\"\\nMots uniques dans les tokens nettoyés: {unique_original}\")\n",
    "print(f\"Mots uniques après stemming: {unique_stemmed} (réduction de {((unique_original - unique_stemmed) / unique_original * 100):.1f}%)\")\n",
    "print(f\"Mots uniques après lemmatisation: {unique_lemmatized} (réduction de {((unique_original - unique_lemmatized) / unique_original * 100):.1f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
